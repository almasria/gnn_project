{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from torch.optim import LBFGS, Adam\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# make sure that util is correctly accessed from parent directory\n",
    "ppp_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if ppp_dir not in sys.path:\n",
    "    sys.path.insert(0, ppp_dir)\n",
    "\n",
    "from util import *\n",
    "from model_parametrized.fls_parametrized import FLS_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If CUDA is available, print the CUDA version\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds and configure cuda device\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEData(Dataset):\n",
    "    def __init__(self, t_range, rho_values, t_points, constant_x, device='cuda:0'):\n",
    "        \"\"\"\n",
    "        Initialize the dataset for a logistic growth ODE with a constant spatial coordinate.\n",
    "\n",
    "        Args:\n",
    "            t_range (list): Time domain [t_min, t_max].\n",
    "            rho_values (list): List of reaction coefficients (ρ values).\n",
    "            t_points (int): Number of time points.\n",
    "            constant_x (float): The constant spatial coordinate (e.g., a representative location).\n",
    "            device (str): Device to store the tensors ('cpu' or 'cuda:0').\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.t_range = t_range\n",
    "        self.rho_values = rho_values\n",
    "        self.t_points = t_points\n",
    "        self.constant_x = constant_x\n",
    "\n",
    "        # Prepare data for each rho value.\n",
    "        self.data = {}\n",
    "        for rho in rho_values:\n",
    "            # Generate residual points (time samples with constant x)\n",
    "            res, ic = self._generate_data()\n",
    "            res_tensor = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            ic_tensor = torch.tensor(ic, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            \n",
    "            # Precompute analytical solution at the initial condition (t = t_range[0])\n",
    "            u_ic = self.analytical_solution(\n",
    "                torch.tensor([[constant_x]], dtype=torch.float32, requires_grad=True).to(self.device),\n",
    "                torch.tensor([[t_range[0]]], dtype=torch.float32, requires_grad=True).to(self.device),\n",
    "                rho\n",
    "            )\n",
    "            \n",
    "            self.data[rho] = {\n",
    "                'res': res_tensor,   # (x, t) pairs over the time domain (x is constant)\n",
    "                'ic': ic_tensor,     # Initial condition point (t = t_range[0])\n",
    "                'u_ic': u_ic         # Analytical solution at t = t_range[0]\n",
    "            }\n",
    "\n",
    "    def _generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate residual points (for the interior of the time domain) and the initial condition.\n",
    "        \n",
    "        Returns:\n",
    "            res (np.ndarray): Array of shape (t_points, 2) where each row is [constant_x, t].\n",
    "            ic (np.ndarray): Array of shape (1, 2) corresponding to the initial condition at t = t_range[0].\n",
    "        \"\"\"\n",
    "        # Create time samples\n",
    "        t = np.linspace(self.t_range[0], self.t_range[1], self.t_points)\n",
    "        # For each t, x is always the constant value provided.\n",
    "        x = self.constant_x * np.ones_like(t)\n",
    "        # Stack x and t to create our (x,t) pairs.\n",
    "        res = np.stack([x, t], axis=-1)  # Shape: (t_points, 2)\n",
    "        # The initial condition is simply the first row.\n",
    "        ic = res[0:1, :]\n",
    "        return res, ic\n",
    "\n",
    "    def analytical_solution(self, x, t, rho):\n",
    "        \"\"\"\n",
    "        Compute the analytical solution for the logistic growth ODE.\n",
    "        Here we use the same functional form as before:\n",
    "        \n",
    "        u(t) = h(x) * exp(ρ t) / (h(x) * exp(ρ t) + 1 - h(x)),  with\n",
    "        h(x) = exp( - (x - π)² / [2*(π/4)²] ).\n",
    "        \n",
    "        Note: Since x is constant, h(x) is also constant.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The spatial input (constant value).\n",
    "            t (torch.Tensor): Time input.\n",
    "            rho (float): Reaction coefficient.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The analytical solution.\n",
    "        \"\"\"\n",
    "        pi = torch.tensor(np.pi, dtype=torch.float32, device=self.device)\n",
    "        h = torch.exp(- (x - pi)**2 / (2 * (pi / 4)**2))\n",
    "        return h * torch.exp(rho * t) / (h * torch.exp(rho * t) + 1 - h)\n",
    "\n",
    "    def get_interior_points(self, rho):\n",
    "        \"\"\"\n",
    "        Retrieve the interior (residual) points for a given rho.\n",
    "        \n",
    "        Returns:\n",
    "            x (torch.Tensor): Spatial component (constant).\n",
    "            t (torch.Tensor): Temporal component.\n",
    "            rho_tensor (torch.Tensor): Tensor filled with the rho value.\n",
    "        \"\"\"\n",
    "        res = self.data[rho]['res']\n",
    "        x = res[:, 0:1]\n",
    "        t = res[:, 1:2]\n",
    "        rho_tensor = torch.full_like(x, rho)\n",
    "        return x, t, rho_tensor\n",
    "\n",
    "    def get_initial_condition(self, rho):\n",
    "        \"\"\"\n",
    "        Retrieve the initial condition point and its analytical solution.\n",
    "        \n",
    "        Returns:\n",
    "            x_ic, t_ic (torch.Tensor): The initial (x, t) point.\n",
    "            rho_tensor (torch.Tensor): Tensor filled with the rho value.\n",
    "            u_ic (torch.Tensor): The precomputed analytical solution at t = t_range[0].\n",
    "        \"\"\"\n",
    "        ic = self.data[rho]['ic']\n",
    "        x_ic = ic[:, 0:1]\n",
    "        t_ic = ic[:, 1:2]\n",
    "        rho_tensor = torch.full_like(x_ic, rho)\n",
    "        u_ic = self.data[rho]['u_ic']\n",
    "        return x_ic, t_ic, rho_tensor, u_ic\n",
    "\n",
    "    def get_test_points(self, rho):\n",
    "        \"\"\"\n",
    "        For this simple ODE experiment, the test points are the same as the interior points.\n",
    "        \n",
    "        Returns:\n",
    "            x, t, rho tensor.\n",
    "        \"\"\"\n",
    "        return self.get_interior_points(rho)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### setting configuration parameters and logging via wandb\n",
    "total_i = 500\n",
    "model_name = \"fls_extended\" \n",
    "in_dim = 3 \n",
    "hidden_dim = 256 #64 #512 \n",
    "out_dim=1\n",
    "num_layer= 4 #2#4\n",
    "bias_fill = 0.01\n",
    "\n",
    "config_dict = {\n",
    "    \"total_i\": total_i,\n",
    "    \"dataset\": \"1d-logistic-ode\",\n",
    "    \"model\": model_name,\n",
    "    \"in_dim\": in_dim,\n",
    "    \"hidden_dim\": hidden_dim, \n",
    "    \"out_dim\": out_dim, \n",
    "    \"num_layer\": num_layer,\n",
    "    \"init_weights\": \"xavier uniform\", \n",
    "    \"bias_fill\": bias_fill,\n",
    "    \"optimizer\": \"adam\", \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128\n",
    "}\n",
    "\n",
    "\"\"\" run = wandb. init(\n",
    "    project=\"pinnsformer\",\n",
    "    config=config_dict,\n",
    "    settings=wandb.Settings(silent=True)\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PINNs \n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(config_dict[\"bias_fill\"])\n",
    "        \n",
    "model = FLS_params(in_dim=config_dict[\"in_dim\"], hidden_dim=config_dict[\"hidden_dim\"], out_dim=config_dict[\"out_dim\"], num_layer=config_dict[\"num_layer\"]).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "#model.apply(init_weights)\n",
    "if config_dict[\"optimizer\"] == \"LBFGS\":\n",
    "    optim = LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "elif config_dict[\"optimizer\"] == \"adam\":\n",
    "    optim = Adam(model.parameters(), lr=config_dict[\"learning_rate\"])\n",
    "\n",
    "print(model)\n",
    "print(get_n_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rho values\n",
    "rho_values = [0.5, 0.7, 0.8] #-> error: 0.05\n",
    "rho_values = [0.5, 4, 5, 10.0] #-> error: 2,5\n",
    "total_i = config_dict[\"total_i\"]\n",
    "\n",
    "# Create the dataset using the ODEData class.\n",
    "# Note: We now provide t_range, t_points, a constant x value (e.g., 1.0), and rho_values.\n",
    "dataset = ODEData(t_range=[0, 1], rho_values=rho_values, t_points=101, constant_x=1.0, device='cuda:0')\n",
    "\n",
    "# %% Combined Training Loop for Multiple ρ Values\n",
    "\n",
    "# Assume dataset.t_points is defined (e.g., 101)\n",
    "t_points = dataset.t_points\n",
    "\n",
    "loss_track = {}  # we'll store per-iteration per-ρ losses in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_dict[\"optimizer\"] == \"LBFGS\":\n",
    "\n",
    "    for i in tqdm(range(total_i)):\n",
    "        def closure():\n",
    "            all_x_res, all_t_res, all_rho_res = [], [], []\n",
    "            all_x_ic, all_t_ic, all_rho_ic, all_u_ic = [], [], [], []\n",
    "            \n",
    "            # Loop over each ρ value and accumulate data\n",
    "            for rho in rho_values:\n",
    "                # Get interior points for the current ρ\n",
    "                x_res, t_res, rho_res = dataset.get_interior_points(rho)\n",
    "                # Force gradients for residual points\n",
    "                x_res = x_res.clone().detach().requires_grad_(True)\n",
    "                t_res = t_res.clone().detach().requires_grad_(True)\n",
    "                rho_res = rho_res.clone().detach().requires_grad_(True)\n",
    "                \n",
    "                all_x_res.append(x_res)\n",
    "                all_t_res.append(t_res)\n",
    "                all_rho_res.append(rho_res)\n",
    "                \n",
    "                # Get initial condition for the current ρ\n",
    "                x_ic, t_ic, rho_ic, u_ic = dataset.get_initial_condition(rho)\n",
    "                all_x_ic.append(x_ic)\n",
    "                all_t_ic.append(t_ic)\n",
    "                all_rho_ic.append(rho_ic)\n",
    "                all_u_ic.append(u_ic)\n",
    "            \n",
    "            # Concatenate data from all ρ values along the batch dimension.\n",
    "            x_res_all = torch.cat(all_x_res, dim=0)\n",
    "            t_res_all = torch.cat(all_t_res, dim=0)\n",
    "            rho_res_all = torch.cat(all_rho_res, dim=0)\n",
    "            \n",
    "            x_ic_all = torch.cat(all_x_ic, dim=0)\n",
    "            t_ic_all = torch.cat(all_t_ic, dim=0)\n",
    "            rho_ic_all = torch.cat(all_rho_ic, dim=0)\n",
    "            u_ic_all = torch.cat(all_u_ic, dim=0)\n",
    "            \n",
    "            # Model predictions on all interior and initial points.\n",
    "            pred_res = model(x_res_all, t_res_all, rho_res_all)\n",
    "            pred_ic = model(x_ic_all, t_ic_all, rho_ic_all)\n",
    "            \n",
    "            # Compute time derivative for the entire residual batch.\n",
    "            u_t = torch.autograd.grad(pred_res, t_res_all,\n",
    "                                    grad_outputs=torch.ones_like(pred_res),\n",
    "                                    retain_graph=True, create_graph=True)[0]\n",
    "            \n",
    "            # Compute aggregated loss on the full batch.\n",
    "            loss_res = torch.mean((u_t - rho_res_all * pred_res * (1 - pred_res)) ** 2)\n",
    "            loss_ic = torch.mean((pred_ic - u_ic_all) ** 2)\n",
    "            loss = loss_res + loss_ic\n",
    "            \n",
    "            # Backward pass\n",
    "            optim.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss  # Return only the total loss\n",
    "\n",
    "        # Perform optimization step\n",
    "        loss = optim.step(closure)\n",
    "\n",
    "        \"\"\" # Recompute additional values outside the closure\n",
    "        with torch.no_grad():\n",
    "            # Compute per-ρ losses by slicing the batch\n",
    "            per_rho_losses = {}\n",
    "            for idx, rho in enumerate(rho_values):\n",
    "                # Each ρ contributed t_points samples for the interior.\n",
    "                start = idx * t_points\n",
    "                end = start + t_points\n",
    "                # Compute loss for residual in this group:\n",
    "                loss_res_rho = torch.mean((u_t[start:end] - rho * pred_res[start:end] * (1 - pred_res[start:end]))**2)\n",
    "                # For the initial condition, each group has one sample (assumed order preserved)\n",
    "                loss_ic_rho = torch.mean((pred_ic[idx:idx+1] - u_ic_all[idx:idx+1])**2)\n",
    "                per_rho_losses[rho] = {\"loss_res\": loss_res_rho.item(), \"loss_ic\": loss_ic_rho.item()} \"\"\"\n",
    "\n",
    "        # Log aggregated losses\n",
    "        wandb.log({\"loss\": loss.item(), \"iteration\": i})\n",
    "        \n",
    "\n",
    "        # Store per-ρ losses for later printing/analysis\n",
    "        per_rho_losses = {}\n",
    "        rho = -1\n",
    "        per_rho_losses[rho] = {\"loss_res\": -1, \"loss_ic\": -1}\n",
    "        loss_track[i] = per_rho_losses\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_errors(model, dataset, rho, device):\n",
    "    \n",
    "    # Get test points for the current rho\n",
    "    x_test, t_test, _ = dataset.get_test_points(rho)\n",
    "    rho_test = torch.full_like(x_test, rho).to(device)  # Create rho tensor for test points\n",
    "\n",
    "    # Compute the analytical solution for the test points\n",
    "    u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(101, 1)\n",
    "\n",
    "    # Model predictions\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "        pred = pred.cpu().detach().numpy().reshape(101, 1)\n",
    "\n",
    "    # Compute relative errors\n",
    "    rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
    "    rl2 = np.sqrt(np.sum((u_analytical - pred) ** 2) / np.sum(u_analytical ** 2))\n",
    "\n",
    "    return rl1, rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define rho values\n",
    "#rho_values = [0.5, 1.0, 2.0, 4.0, 10.0]\n",
    "#total_i = config_dict[\"total_i\"]\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "normalize_res_values = [True, False]\n",
    "normalize_ic_values = [True, False]\n",
    "alpha_values = [0.1, 0.25, 0.5]\n",
    "epsilon_values = [0.1, 0.5, 1.0]\n",
    "iteration_steps = [50]  # Different values for total_i\n",
    "\n",
    "# Hyperparameter tuning loop\n",
    "for normalize_res in normalize_res_values:\n",
    "    for normalize_ic in normalize_ic_values:\n",
    "        for alpha in alpha_values:\n",
    "            for epsilon in epsilon_values:\n",
    "                for total_i in iteration_steps:\n",
    "                    # Initialize a new wandb run for each combination of hyperparameters\n",
    "\n",
    "                    config_dict[\"normalize_res\"] = normalize_res\n",
    "                    config_dict[\"normalize_ic\"] = normalize_ic\n",
    "                    config_dict[\"alpha\"] = alpha\n",
    "                    config_dict[\"epsilon\"] = epsilon\n",
    "                    config_dict[\"total_i\"] = total_i\n",
    "                    \n",
    "                    wandb.init(\n",
    "                        project=\"gnn_1d_logistic\",\n",
    "                        config=config_dict,\n",
    "                        settings=wandb.Settings(silent=True)\n",
    "                    )\n",
    "                    #config = wandb.config\n",
    "\n",
    "                    # Training loop\n",
    "                    loss_track = {}  # Initialize a dictionary for tracking losses\n",
    "\n",
    "                    for i in tqdm(range(config_dict[\"total_i\"])):\n",
    "                        total_loss_res = 0.0\n",
    "                        total_loss_ic = 0.0\n",
    "                        num_batches = 0  # Track the number of batches\n",
    "\n",
    "                        # Loop over each ρ value\n",
    "                        for rho in rho_values:\n",
    "                            # Get interior points and create a mini-batch DataLoader\n",
    "                            x_res, t_res, rho_res = dataset.get_interior_points(rho)\n",
    "                            interior_dataset = TensorDataset(x_res, t_res, rho_res)\n",
    "                            interior_loader = DataLoader(interior_dataset, batch_size=config_dict[\"batch_size\"], shuffle=True)\n",
    "\n",
    "                            # Get initial condition points\n",
    "                            x_ic, t_ic, rho_ic, u_ic = dataset.get_initial_condition(rho)\n",
    "\n",
    "                            for bx, bt, brho in interior_loader:\n",
    "                                # Ensure mini-batch inputs require gradients for differentiation\n",
    "                                bx.requires_grad_()\n",
    "                                bt.requires_grad_()\n",
    "\n",
    "                                # Forward pass on interior mini-batch\n",
    "                                pred_res = model(bx.to(device), bt.to(device), brho.to(device))\n",
    "\n",
    "                                # Compute time derivative for residual points\n",
    "                                u_t = torch.autograd.grad(pred_res, bt.to(device),\n",
    "                                                          grad_outputs=torch.ones_like(pred_res),\n",
    "                                                          retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "                                # Compute residual loss\n",
    "                                if config_dict[\"normalize_res\"]:\n",
    "                                    normalized_res_error = (u_t - brho * pred_res * (1 - pred_res)) / (config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
    "                                    loss_res = torch.mean(normalized_res_error ** 2)\n",
    "                                else:\n",
    "                                    loss_res = torch.mean((u_t - brho * pred_res * (1 - pred_res)) ** 2)\n",
    "\n",
    "                                # Forward pass for initial condition points\n",
    "                                pred_ic = model(x_ic.to(device), t_ic.to(device), rho_ic.to(device))\n",
    "\n",
    "                                # Compute initial condition loss\n",
    "                                if config_dict[\"normalize_ic\"]:\n",
    "                                    normalized_ic_error = (pred_ic - u_ic.to(device)) / (config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
    "                                    loss_ic = torch.mean(normalized_ic_error ** 2)\n",
    "                                else:\n",
    "                                    loss_ic = torch.mean((pred_ic - u_ic.to(device)) ** 2)\n",
    "\n",
    "                                # Total loss\n",
    "                                loss = loss_res + loss_ic\n",
    "\n",
    "                                # Backward pass and optimization step\n",
    "                                optim.zero_grad()\n",
    "                                loss.backward(retain_graph=True)\n",
    "                                optim.step()\n",
    "\n",
    "                                # Accumulate losses for averaging\n",
    "                                total_loss_res += loss_res.item()\n",
    "                                total_loss_ic += loss_ic.item()\n",
    "                                num_batches += 1\n",
    "\n",
    "                        # Compute average losses\n",
    "                        avg_loss_res = total_loss_res / num_batches\n",
    "                        avg_loss_ic = total_loss_ic / num_batches\n",
    "                        avg_total_loss = avg_loss_res + avg_loss_ic\n",
    "\n",
    "\n",
    "                        wandb_dict = {\n",
    "                            \"iteration\": i,\n",
    "                            \"avg_loss_res\": avg_loss_res,\n",
    "                            \"avg_loss_ic\": avg_loss_ic,\n",
    "                            \"avg_total_loss\": avg_total_loss\n",
    "                        }\n",
    "\n",
    "                        for rho in rho_values: \n",
    "                            rl1, rl2 = compute_relative_errors(model, dataset, rho, device)\n",
    "                            wandb_dict[f\"{rho}_rl1\"] = rl1\n",
    "                            wandb_dict[f\"{rho}_rl2\"] = rl2\n",
    "\n",
    "                        # Log losses\n",
    "                        wandb.log(wandb_dict)\n",
    "\n",
    "                        # Store losses for later analysis\n",
    "                        loss_track[i] = {\n",
    "                            \"avg_loss_res\": avg_loss_res,\n",
    "                            \"avg_loss_ic\": avg_loss_ic,\n",
    "                            \"avg_total_loss\": avg_total_loss\n",
    "                        }\n",
    "\n",
    "                    # Finish the wandb run\n",
    "                    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "# %%\n",
    "base_dir = Path(\".\")  # Base directory for results\n",
    "images_dir = base_dir / \"images\"  # Subdirectory for images\n",
    "weights_dir = base_dir / \"weights\"  # Subdirectory for stored model\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    " # %%\n",
    "# Print losses for the last iteration for each rho.\n",
    "# Note: We no longer have boundary (BC) losses, so we only print residual (Res) and initial condition (IC) losses.\n",
    "last_iteration = total_i - 1\n",
    "for _ in range(1):\n",
    "    print('Loss Res: {:4f}, Loss IC: {:4f}'.format(\n",
    "        loss_track[last_iteration][\"avg_loss_res\"],\n",
    "        loss_track[last_iteration][\"avg_loss_ic\"]\n",
    "    ))\n",
    "    print('Train Loss: {:4f}'.format(\n",
    "        loss_track[last_iteration][\"avg_loss_res\"] +\n",
    "        loss_track[last_iteration][\"avg_loss_ic\"]\n",
    "    )) \n",
    "\n",
    "# Save the model with a filename reflecting the 1D logistic ODE experiment.\n",
    "model_path = weights_dir / \"1d_logistic_ode_fls_extended.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Initialize storage for errors\n",
    "errors = []\n",
    "\n",
    "# Loop over each rho value for testing.\n",
    "# Note: The ODE dataset returns only (x, t, rho) from get_test_points.\n",
    "for rho in rho_values:\n",
    "    # Get test points for the current rho (x_test and t_test are 101x1 tensors)\n",
    "    x_test, t_test, _ = dataset.get_test_points(rho)\n",
    "    rho_test = torch.full_like(x_test, rho).to(device)  # Create rho tensor for test points\n",
    "\n",
    "    # Compute the analytical solution for the test points.\n",
    "    # Since the problem is 1D in time, reshape to (101, 1) rather than a 2D grid.\n",
    "    u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(101, 1)\n",
    "\n",
    "    # Model predictions.\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "        pred = pred.cpu().detach().numpy().reshape(101, 1)\n",
    "\n",
    "    # Compute relative errors.\n",
    "    rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
    "    rl2 = np.sqrt(np.sum((u_analytical - pred) ** 2) / np.sum(u_analytical ** 2))\n",
    "\n",
    "    print(f\"Rho: {rho}, Relative L1 error: {rl1:.4f}, Relative L2 error: {rl2:.4f}\")\n",
    "    errors.append((rho, rl1, rl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Visualization for each rho value using line plots (since data is 1D in time).\n",
    "for rho in rho_values:\n",
    "    # Get test points for the current rho.\n",
    "    x_test, t_test, _ = dataset.get_test_points(rho)\n",
    "    rho_test = torch.full_like(x_test, rho).to(device)\n",
    "\n",
    "    # Compute the analytical solution.\n",
    "    u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(101, 1)\n",
    "\n",
    "    # Model predictions.\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "        pred = pred.cpu().detach().numpy().reshape(101, 1)\n",
    "\n",
    "    # Absolute error.\n",
    "    abs_error = np.abs(u_analytical - pred)\n",
    "    \n",
    "    # Extract time values for plotting (1D array of shape (101,)).\n",
    "    t_vals = t_test.cpu().detach().numpy().flatten()\n",
    "\n",
    "    # Plot the predicted and exact solutions.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(t_vals, pred, label='Predicted', marker='o')\n",
    "    plt.plot(t_vals, u_analytical, label='Exact', linestyle='--')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('u(t)')\n",
    "    plt.title(f'Comparison of u(t) for rho={rho}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    # Updated file name for logistic ODE.\n",
    "    plot_path = images_dir / f\"1d_logistic_ode_fls_comparison_rho-{rho}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the absolute error.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(t_vals, abs_error, label='Absolute Error', marker='o')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(f'Absolute Error in u(t) for rho={rho}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_path = images_dir / f\"1d_logistic_ode_fls_error_rho-{rho}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
