{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U25MXCU8EUvn",
        "outputId": "37215ed5-14a6-4632-88e9-b3ca404975ff"
      },
      "outputs": [],
      "source": [
        "# install wandb in colab (if required)\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "p_I5HPhLEUvp",
        "outputId": "0dd49718-8065-4571-8f41-4bc9800abbff"
      },
      "outputs": [],
      "source": [
        "# login (if required)\n",
        "import wandb\n",
        "\n",
        "### REGIME !!!\n",
        "regime_str = \"low_regime\"\n",
        "\n",
        "### WANDB PROJECT NAME !!!\n",
        "wandb_project_name = f\"gnn_1d_logistic_pinnsformer_{regime_str}\"\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RaVc_Z7EUvp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import copy\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import random\n",
        "from torch.optim import LBFGS, Adam\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# make sure that util is correctly accessed from parent directory\n",
        "ppp_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
        "if ppp_dir not in sys.path:\n",
        "    sys.path.insert(0, ppp_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8o2d_xpEUvq"
      },
      "outputs": [],
      "source": [
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class WaveAct(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WaveAct, self).__init__()\n",
        "        self.w1 = nn.Parameter(torch.ones(1), requires_grad=True)\n",
        "        self.w2 = nn.Parameter(torch.ones(1), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w1 * torch.sin(x)+ self.w2 * torch.cos(x)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=256):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear = nn.Sequential(*[\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            WaveAct(),\n",
        "            nn.Linear(d_ff, d_ff),\n",
        "            WaveAct(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.act1 = WaveAct()\n",
        "        self.act2 = WaveAct()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x2 = self.act1(x)\n",
        "        # pdb.set_trace()\n",
        "        x = x + self.attn(x2,x2,x2)[0]\n",
        "        x2 = self.act2(x)\n",
        "        x = x + self.ff(x2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.act1 = WaveAct()\n",
        "        self.act2 = WaveAct()\n",
        "\n",
        "    def forward(self, x, e_outputs):\n",
        "        x2 = self.act1(x)\n",
        "        x = x + self.attn(x2, e_outputs, e_outputs)[0]\n",
        "        x2 = self.act2(x)\n",
        "        x = x + self.ff(x2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, N, heads):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.N = N\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.act = WaveAct()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x)\n",
        "        return self.act(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, N, heads):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.N = N\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.act = WaveAct()\n",
        "\n",
        "    def forward(self, x, e_outputs):\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs)\n",
        "        return self.act(x)\n",
        "\n",
        "\n",
        "class PINNsformer(nn.Module):\n",
        "    def __init__(self, d_out, d_model, d_hidden, N, heads):\n",
        "        \"\"\"\n",
        "        Adapted PINNsformer that takes three inputs: x, t, and rho.\n",
        "        Args:\n",
        "            d_out (int): Output dimension.\n",
        "            d_model (int): Dimension of the model embeddings.\n",
        "            d_hidden (int): Hidden layer dimension in the output MLP.\n",
        "            N (int): Number of encoder/decoder layers.\n",
        "            heads (int): Number of attention heads.\n",
        "        \"\"\"\n",
        "        super(PINNsformer, self).__init__()\n",
        "        # Change input dimension from 2 to 3 to accommodate x, t, and rho\n",
        "        self.linear_emb = nn.Linear(3, d_model)\n",
        "\n",
        "        self.encoder = Encoder(d_model, N, heads)\n",
        "        self.decoder = Decoder(d_model, N, heads)\n",
        "        self.linear_out = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hidden),\n",
        "            WaveAct(),\n",
        "            nn.Linear(d_hidden, d_hidden),\n",
        "            WaveAct(),\n",
        "            nn.Linear(d_hidden, d_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, rho):\n",
        "        # Concatenate x, t, and rho along the last dimension\n",
        "        src = torch.cat((x, t, rho), dim=-1)\n",
        "        src = self.linear_emb(src)\n",
        "\n",
        "        e_outputs = self.encoder(src)\n",
        "        d_output = self.decoder(src, e_outputs)\n",
        "        output = self.linear_out(d_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoJ6p1kdEUvr"
      },
      "outputs": [],
      "source": [
        "class ODEData(Dataset):\n",
        "    def __init__(self, t_range, rho_values, t_points, constant_x, device='cuda:0', use_time_sequencing=False):\n",
        "        \"\"\"\n",
        "        Initialize the dataset for a logistic growth ODE with a constant spatial coordinate.\n",
        "\n",
        "        Args:\n",
        "            t_range (list): Time domain [t_min, t_max].\n",
        "            rho_values (list): List of reaction coefficients (? values).\n",
        "            t_points (int): Number of time points.\n",
        "            constant_x (float): The constant spatial coordinate (e.g., a representative location).\n",
        "            device (str): Device to store the tensors ('cpu' or 'cuda:0').\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.t_range = t_range\n",
        "        self.rho_values = rho_values\n",
        "        self.t_points = t_points\n",
        "        self.constant_x = constant_x\n",
        "        self.use_time_sequencing = use_time_sequencing\n",
        "\n",
        "        # Prepare data for each rho value.\n",
        "        self.data = {}\n",
        "        for rho in rho_values:\n",
        "            # Generate residual points (time samples with constant x)\n",
        "\n",
        "            res, ic = self._generate_data()\n",
        "            if self.use_time_sequencing == True:\n",
        "                res = self._make_time_sequence(res)\n",
        "                ic = self._make_time_sequence(ic)\n",
        "\n",
        "            res_tensor = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device)\n",
        "            ic_tensor = torch.tensor(ic, dtype=torch.float32, requires_grad=True).to(self.device)\n",
        "\n",
        "            # Precompute analytical solution at the initial condition (t = t_range[0])\n",
        "            u_ic = self.analytical_solution(\n",
        "                torch.tensor([[constant_x]], dtype=torch.float32, requires_grad=True).to(self.device),\n",
        "                torch.tensor([[t_range[0]]], dtype=torch.float32, requires_grad=True).to(self.device),\n",
        "                rho\n",
        "            )\n",
        "\n",
        "            self.data[rho] = {\n",
        "                'res': res_tensor,   # (x, t) pairs over the time domain (x is constant)\n",
        "                'ic': ic_tensor,     # Initial condition point (t = t_range[0])\n",
        "                'u_ic': u_ic         # Analytical solution at t = t_range[0]\n",
        "            }\n",
        "\n",
        "    def _generate_data(self):\n",
        "        \"\"\"\n",
        "        Generate residual points (for the interior of the time domain) and the initial condition.\n",
        "\n",
        "        Returns:\n",
        "            res (np.ndarray): Array of shape (t_points, 2) where each row is [constant_x, t].\n",
        "            ic (np.ndarray): Array of shape (1, 2) corresponding to the initial condition at t = t_range[0].\n",
        "        \"\"\"\n",
        "        # Create time samples\n",
        "        t = np.linspace(self.t_range[0], self.t_range[1], self.t_points)\n",
        "        # For each t, x is always the constant value provided.\n",
        "        x = self.constant_x * np.ones_like(t)\n",
        "        # Stack x and t to create our (x,t) pairs.\n",
        "        res = np.stack([x, t], axis=-1)  # Shape: (t_points, 2)\n",
        "        # The initial condition is simply the first row.\n",
        "        ic = res[0:1, :]\n",
        "        return res, ic\n",
        "\n",
        "    def analytical_solution(self, x, t, rho):\n",
        "        \"\"\"\n",
        "        Compute the analytical solution for the logistic growth ODE.\n",
        "        Here we use the same functional form as before:\n",
        "\n",
        "        u(t) = h(x) * exp(? t) / (h(x) * exp(? t) + 1 - h(x)),  with\n",
        "        h(x) = exp( - (x - ?)? / [2*(?/4)?] ).\n",
        "\n",
        "        Note: Since x is constant, h(x) is also constant.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The spatial input (constant value).\n",
        "            t (torch.Tensor): Time input.\n",
        "            rho (float): Reaction coefficient.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The analytical solution.\n",
        "        \"\"\"\n",
        "        pi = torch.tensor(np.pi, dtype=torch.float32, device=self.device)\n",
        "        h = torch.exp(- (x - pi)**2 / (2 * (pi / 4)**2))\n",
        "        return h * torch.exp(rho * t) / (h * torch.exp(rho * t) + 1 - h)\n",
        "\n",
        "    def get_test_points(self, rho):\n",
        "        \"\"\"\n",
        "        For this simple ODE experiment, the test points are the same as the interior points.\n",
        "\n",
        "        Returns:\n",
        "            x, t, rho tensor.\n",
        "        \"\"\"\n",
        "        return self.get_interior_points(rho)\n",
        "\n",
        "    def _make_time_sequence(self, src, num_step=5, step=1e-4):\n",
        "        dim = num_step\n",
        "        src = np.repeat(np.expand_dims(src, axis=1), dim, axis=1)  # (N, L, 2)\n",
        "        for i in range(num_step):\n",
        "            src[:,i,-1] += step*i\n",
        "        return src\n",
        "\n",
        "    def get_interior_points(self, rho):\n",
        "        \"\"\"\n",
        "        Retrieve the interior (residual) points for a given rho.\n",
        "        \"\"\"\n",
        "        res = self.data[rho]['res']\n",
        "        if self.use_time_sequencing:\n",
        "            x = res[:, :, 0:1]  # Shape: (N, num_step, 1)\n",
        "            t = res[:, :, 1:2]  # Shape: (N, num_step, 1)\n",
        "        else:\n",
        "            x = res[:, 0:1]\n",
        "            t = res[:, 1:2]\n",
        "\n",
        "        # Create appropriate rho tensor\n",
        "        if self.use_time_sequencing:\n",
        "            rho_tensor = torch.full_like(x, rho)\n",
        "        else:\n",
        "            rho_tensor = torch.full_like(x, rho)\n",
        "\n",
        "        return x, t, rho_tensor\n",
        "\n",
        "    def get_initial_condition(self, rho):\n",
        "        \"\"\"\n",
        "        Retrieve the initial condition point and its analytical solution.\n",
        "        \"\"\"\n",
        "        ic = self.data[rho]['ic']\n",
        "        if self.use_time_sequencing:\n",
        "            x_ic = ic[:, :, 0:1]  # Shape: (1, num_step, 1)\n",
        "            t_ic = ic[:, :, 1:2]  # Shape: (1, num_step, 1)\n",
        "        else:\n",
        "            x_ic = ic[:, 0:1]\n",
        "            t_ic = ic[:, 1:2]\n",
        "\n",
        "        # Create appropriate rho tensor\n",
        "        if self.use_time_sequencing:\n",
        "            rho_tensor = torch.full_like(x_ic, rho)\n",
        "        else:\n",
        "            rho_tensor = torch.full_like(x_ic, rho)\n",
        "\n",
        "        u_ic = self.data[rho]['u_ic']\n",
        "\n",
        "        return x_ic, t_ic, rho_tensor, u_ic\n",
        "\n",
        "    def get_interior_input_without_points(self):\n",
        "        \"\"\"\n",
        "        Generate interior input points dynamically without precomputed values.\n",
        "        \"\"\"\n",
        "        res, _ = self._generate_data()\n",
        "\n",
        "        if self.use_time_sequencing:\n",
        "            res = self._make_time_sequence(res)\n",
        "            res_tensor = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device)\n",
        "            x = res_tensor[:, :, 0:1]  # Shape: (N, num_step, 1)\n",
        "            t = res_tensor[:, :, 1:2]  # Shape: (N, num_step, 1)\n",
        "        else:\n",
        "            res_tensor = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device)\n",
        "            x = res_tensor[:, 0:1]\n",
        "            t = res_tensor[:, 1:2]\n",
        "\n",
        "        return x, t, None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hco5cBFtEUvr"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class HyperparamConfig:\n",
        "    # General parameters\n",
        "    total_i: int = None\n",
        "    dataset: str = None\n",
        "    model_name: str = None\n",
        "    in_dim: int = None\n",
        "    hidden_dim: int = None\n",
        "    out_dim: int = None\n",
        "    num_layer: int = None\n",
        "    init_weights: str = None\n",
        "    bias_fill: float = None\n",
        "\n",
        "    # Model-specific parameters (only relevant if model_name is \"pinn_ff\")\n",
        "    init_activ_func: Optional[str] = None  # Only relevant for \"pinn_ff\"       # sin , tanh, gelu\n",
        "    subseq_activ_func: Optional[str] = None  # Only relevant for \"pinn_ff\"     # tanh, gelu\n",
        "\n",
        "    # Only relevant if model_name is \"pinnsformer\"\n",
        "    emb_dim: Optional[int] = field(default=None)\n",
        "    num_heads: Optional[int] = field(default=None)\n",
        "\n",
        "    # Optimizer parameters\n",
        "    optimizer: str = \"adam\"  # Options: \"adam\", \"lbfgs\"\n",
        "    learning_rate: Optional[float] = field(default=None)  # Only relevant for Adam\n",
        "    batch_size: Optional[int] = field(default=None)  # Only relevant for Adam\n",
        "\n",
        "    # LBFGS-specific parameters\n",
        "    max_iter: Optional[int] = field(default=None)  # Only relevant for LBFGS\n",
        "    line_search_fn: Optional[str] = field(default=None)  # Only relevant for LBFGS\n",
        "\n",
        "    # Normalization parameters\n",
        "    normalize_res: bool = False\n",
        "    normalize_ic: bool = False\n",
        "    alpha: Optional[float] = None  # Only relevant if normalize_res or normalize_ic is True\n",
        "    epsilon: Optional[float] = None  # Only relevant if normalize_res or normalize_ic is True\n",
        "\n",
        "    # Adaptive loss weighting\n",
        "    adaptive_loss_weighting: bool = False\n",
        "    adaptive_loss_coeff: Optional[float] = field(default=None)\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"\n",
        "        Validate the configuration to ensure all conditional parameters are set correctly.\n",
        "        \"\"\"\n",
        "        # Validate optimizer-specific parameters\n",
        "        if self.optimizer == \"adam\":\n",
        "            if self.learning_rate is None:\n",
        "                raise ValueError(\"`learning_rate` must be specified when optimizer is 'adam'.\")\n",
        "            if self.batch_size is None:\n",
        "                raise ValueError(\"`batch_size` must be specified when optimizer is 'adam'.\")\n",
        "        elif self.optimizer == \"lbfgs\":\n",
        "            if self.max_iter is None:\n",
        "                raise ValueError(\"`max_iter` must be specified when optimizer is 'lbfgs'.\")\n",
        "            if self.line_search_fn is None:\n",
        "                raise ValueError(\"`line_search_fn` must be specified when optimizer is 'lbfgs'.\")\n",
        "\n",
        "        # Validate model-specific parameters\n",
        "        if self.model_name == \"pinn_ff\":\n",
        "            if self.init_activ_func is None:\n",
        "                raise ValueError(\"`init_activ_func` must be specified when model_name is 'pinn_ff'.\")\n",
        "            if self.subseq_activ_func is None:\n",
        "                raise ValueError(\"`subseq_activ_func` must be specified when model_name is 'pinn_ff'.\")\n",
        "\n",
        "        if self.model_name == \"pinnsformer\":\n",
        "            if self.emb_dim is None:\n",
        "                raise ValueError(\"`emb_dim` must be specified when model_name is 'pinnsformer'.\")\n",
        "            if self.num_heads is None:\n",
        "                raise ValueError(\"`num_heads` must be specified when model_name is 'pinnsformer'.\")\n",
        "\n",
        "\n",
        "        # Validate normalization-specific parameters\n",
        "        if self.normalize_res or self.normalize_ic:\n",
        "            if self.alpha is None:\n",
        "                raise ValueError(\"`alpha` must be specified when normalize_res or normalize_ic is True.\")\n",
        "            if self.epsilon is None:\n",
        "                raise ValueError(\"`epsilon` must be specified when normalize_res or normalize_ic is True.\")\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"\n",
        "        Convert the dataclass to a dictionary for use with wandb or other logging tools.\n",
        "        \"\"\"\n",
        "        return {k: v for k, v in self.__dict__.items() if v is not None}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEQvH7mGEUvs"
      },
      "outputs": [],
      "source": [
        "base_dir = Path(\".\") / f\"{regime_str}\" # Base directory for results\n",
        "images_dir = base_dir / \"images\"  # Subdirectory for images\n",
        "weights_dir = base_dir / \"weights\"  # Subdirectory for stored model\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "images_dir.mkdir(parents=True, exist_ok=True)\n",
        "weights_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "tdUp4ISZEUvs",
        "outputId": "6ccdb3dd-60c9-4b79-fb6b-25244949cf8f"
      },
      "outputs": [],
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "print(f\"CUDA available: {cuda_available}\")\n",
        "\n",
        "# If CUDA is available, print the CUDA version\n",
        "if cuda_available:\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "    device = 'cuda:0'\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "\"\"\"\n",
        "import torch.backends.cudnn as cudnn\n",
        "torch.use_deterministic_algorithms(True)\n",
        "cudnn.deterministic = True\n",
        "cudnn.benchmark = False\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSgLGRyCEUvt"
      },
      "source": [
        "### Configure initial dataset and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3pe9GEjEUvu",
        "outputId": "56c56b28-872a-4f76-870b-24eb35e95f88"
      },
      "outputs": [],
      "source": [
        "config = HyperparamConfig(\n",
        "    total_i=10,\n",
        "    dataset=\"1d-logistic-ode\",\n",
        "    model_name=\"pinnsformer\",  # Model name\n",
        "    in_dim=3,\n",
        "    hidden_dim=256,\n",
        "    out_dim=1,\n",
        "    emb_dim=32,\n",
        "    num_layer=1,\n",
        "    num_heads=2,\n",
        "    init_weights=\"xavier uniform\",\n",
        "    bias_fill=0.01,\n",
        "    optimizer=\"adam\",\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "# Validate the configuration\n",
        "config.validate()\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(config.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1HMPStEUvv"
      },
      "source": [
        "### 1. Hyperparameter Tuning: Small $\\rho$ parameter variance datasets, adam optimizer with mini batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQnm7zbcEUvv"
      },
      "outputs": [],
      "source": [
        "def generate_rho_values(start, end, num=10):\n",
        "    return np.linspace(start, end, num).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqpwvYFTEUvw"
      },
      "outputs": [],
      "source": [
        "def compute_relative_errors(model, dataset, rho, device):\n",
        "    # Get test points\n",
        "    x, t, rho_tensor = dataset.get_test_points(rho)\n",
        "\n",
        "    # Compute predictions\n",
        "    with torch.no_grad():\n",
        "        pred = model(x, t, rho_tensor).detach().cpu().numpy()\n",
        "\n",
        "    # Compute analytical solution\n",
        "    with torch.no_grad():\n",
        "        u_analytical = dataset.analytical_solution(x, t, rho).detach().cpu().numpy()\n",
        "\n",
        "    # Handle time sequencing case (shapes will be different)\n",
        "    if dataset.use_time_sequencing:\n",
        "        # Option 1: Reshape predictions to match analytical solution\n",
        "        if pred.shape != u_analytical.shape:\n",
        "            if len(pred.shape) == 3:  # If pred has shape (N, num_step, 1)\n",
        "                pred = pred.reshape(-1, 1)  # Flatten to (N*num_step, 1)\n",
        "            if len(u_analytical.shape) == 3:  # If u_analytical has shape (N, num_step, 1)\n",
        "                u_analytical = u_analytical.reshape(-1, 1)  # Flatten to (N*num_step, 1)\n",
        "\n",
        "    # Compute relative errors\n",
        "    rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
        "    rl2 = np.sqrt(np.sum((u_analytical - pred) ** 2) / np.sum(u_analytical ** 2))\n",
        "\n",
        "    return rl1, rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liWT5gu6EUvw"
      },
      "outputs": [],
      "source": [
        "# === Helper Functions ===\n",
        "def compute_grad_norm(loss, model):\n",
        "    \"\"\"Compute and normalize gradient of loss w.r.t. model parameters.\"\"\"\n",
        "    grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
        "    flat_grad = torch.cat([g.view(-1) for g in grads if g is not None])\n",
        "    return flat_grad / (flat_grad.norm() + 1e-8)\n",
        "\n",
        "def update_momentum(grads, momentum_dict, alpha):\n",
        "    \"\"\"Exponential moving average update for gradient momentum.\"\"\"\n",
        "    for key in grads:\n",
        "        if key not in momentum_dict:\n",
        "            momentum_dict[key] = grads[key].detach().clone()\n",
        "        else:\n",
        "            momentum_dict[key] = alpha * momentum_dict[key] + (1 - alpha) * grads[key].detach()\n",
        "    return momentum_dict\n",
        "\n",
        "def compute_loss_weights(momentum_dict):\n",
        "    \"\"\"Compute inverse-norm weights from gradient momenta.\"\"\"\n",
        "    weights = {k: 1.0 / (v.norm() + 1e-8) for k, v in momentum_dict.items()}\n",
        "    total = sum(weights.values())\n",
        "    return {k: weights[k] / total for k in weights}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8YvdWvHEUvx"
      },
      "outputs": [],
      "source": [
        "# === Helper Function for One Training Run ===\n",
        "def run_training(config_dict, model, optim, dataset, rho_values, device):\n",
        "    \"\"\"\n",
        "    Perform one training run based on the provided configuration.\n",
        "\n",
        "    Args:\n",
        "        config_dict (dict): Configuration dictionary containing hyperparameters.\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        dataset: Dataset object providing interior and initial condition points.\n",
        "        rho_values (list): List of rho values for training.\n",
        "        device (torch.device): Device to run the training on (e.g., 'cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    wandb.init(\n",
        "        project=wandb_project_name,\n",
        "        config=config_dict,\n",
        "        settings=wandb.Settings(silent=True)\n",
        "    )\n",
        "\n",
        "    momentum_dict = {}  # Reset for each run\n",
        "    loss_track = {}\n",
        "    model.train() # set to training mode\n",
        "\n",
        "    for i in tqdm(range(config_dict[\"total_i\"])):\n",
        "        total_loss_res = 0.0\n",
        "        total_loss_ic = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for rho in rho_values:\n",
        "            x_res, t_res, rho_res = dataset.get_interior_points(rho)\n",
        "            interior_dataset = TensorDataset(x_res, t_res, rho_res)\n",
        "            interior_loader = DataLoader(interior_dataset, batch_size=config_dict[\"batch_size\"], shuffle=True)\n",
        "\n",
        "            x_ic, t_ic, rho_ic, u_ic = dataset.get_initial_condition(rho)\n",
        "\n",
        "            for bx, bt, brho in interior_loader:\n",
        "                bx.requires_grad_()\n",
        "                bt.requires_grad_()\n",
        "\n",
        "                pred_res = model(bx.to(device), bt.to(device), brho.to(device))\n",
        "                u_t = torch.autograd.grad(pred_res, bt.to(device), grad_outputs=torch.ones_like(pred_res),\n",
        "                                          retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "                if config_dict[\"normalize_res\"]:\n",
        "                    normalized_res_error = (u_t - brho * pred_res * (1 - pred_res)) / (\n",
        "                        config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
        "                    loss_res = torch.mean(normalized_res_error ** 2)\n",
        "                else:\n",
        "                    loss_res = torch.mean((u_t - brho * pred_res * (1 - pred_res)) ** 2)\n",
        "\n",
        "                pred_ic = model(x_ic.to(device), t_ic.to(device), rho_ic.to(device))\n",
        "\n",
        "                if config_dict[\"normalize_ic\"]:\n",
        "                    normalized_ic_error = (pred_ic - u_ic.to(device)) / (\n",
        "                        config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
        "                    loss_ic = torch.mean(normalized_ic_error ** 2)\n",
        "                else:\n",
        "                    loss_ic = torch.mean((pred_ic - u_ic.to(device)) ** 2)\n",
        "\n",
        "                if config_dict[\"adaptive_loss_weighting\"]:\n",
        "                    grad_res = compute_grad_norm(loss_res, model)\n",
        "                    grad_ic = compute_grad_norm(loss_ic, model)\n",
        "\n",
        "                    grads = {'res': grad_res, 'ic': grad_ic}\n",
        "                    momentum_dict = update_momentum(grads, momentum_dict, config_dict[\"adaptive_loss_coeff\"])\n",
        "                    gamma = compute_loss_weights(momentum_dict)\n",
        "\n",
        "                    loss = gamma['res'] * loss_res + gamma['ic'] * loss_ic\n",
        "                else:\n",
        "                    loss = loss_res + loss_ic\n",
        "\n",
        "                optim.zero_grad()\n",
        "                loss.backward(retain_graph=True)\n",
        "                optim.step()\n",
        "\n",
        "                total_loss_res += loss_res.item()\n",
        "                total_loss_ic += loss_ic.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        avg_loss_res = total_loss_res / num_batches\n",
        "        avg_loss_ic = total_loss_ic / num_batches\n",
        "        avg_total_loss = avg_loss_res + avg_loss_ic\n",
        "\n",
        "        wandb_dict = {\n",
        "            \"iteration\": i,\n",
        "            \"avg_loss_res\": avg_loss_res,\n",
        "            \"avg_loss_ic\": avg_loss_ic,\n",
        "            \"avg_total_loss\": avg_total_loss\n",
        "        }\n",
        "\n",
        "        total_l1 = 0.0\n",
        "        total_l2 = 0.0\n",
        "        for rho in rho_values:\n",
        "            rl1, rl2 = compute_relative_errors(model, dataset, rho, device)\n",
        "            wandb_dict[f\"{rho}_rl1\"] = rl1\n",
        "            wandb_dict[f\"{rho}_rl2\"] = rl2\n",
        "            total_l1 += rl1\n",
        "            total_l2 += rl2\n",
        "\n",
        "        average_l1 = total_l1 / len(rho_values)\n",
        "        average_l2 = total_l2 / len(rho_values)\n",
        "\n",
        "        wandb.log(wandb_dict)\n",
        "        loss_track[i] = wandb_dict\n",
        "\n",
        "    wandb.finish()\n",
        "    return loss_track, average_l1, average_l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLgx0GGwEUvx"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(config.bias_fill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUN3cl16EUvx"
      },
      "outputs": [],
      "source": [
        "def plot_predictions_and_errors(rho_values, predictions, analytical_solutions, errors, num_cols=4, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot predictions and errors for multiple rho values in a grid layout.\n",
        "\n",
        "    Args:\n",
        "        rho_values (list): List of rho values.\n",
        "        predictions (dict): Dictionary of predictions for each rho value.\n",
        "        analytical_solutions (dict): Dictionary of analytical solutions for each rho value.\n",
        "        errors (dict): Dictionary of absolute errors for each rho value.\n",
        "        num_cols (int): Number of columns in the grid (default: 4).\n",
        "        save_path (str or Path, optional): Path to save the figure. If None, the figure is not saved.\n",
        "    \"\"\"\n",
        "    num_rho = len(rho_values)\n",
        "    num_rows = 2  # Fixed: Row 1 for predictions, Row 2 for errors\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 6))\n",
        "\n",
        "    # Ensure axes is a 2D array even if num_cols == 1\n",
        "    axes = np.atleast_2d(axes)\n",
        "\n",
        "    for idx, rho in enumerate(rho_values):\n",
        "        col = idx % num_cols\n",
        "\n",
        "        # Extract data for the current rho\n",
        "        pred = predictions[rho]\n",
        "        analytical = analytical_solutions[rho]\n",
        "        abs_error = errors[rho]\n",
        "        percentage_error = (abs_error / np.maximum(analytical, 1e-8)) * 100  # Avoid division by zero\n",
        "\n",
        "        ax_pred = axes[0, col]\n",
        "        ax_pred.plot(pred, label=\"Prediction\", color=\"blue\", linewidth=2)\n",
        "        ax_pred.plot(analytical, label=\"Analytical\", color=\"orange\", linestyle=\"dashed\", linewidth=2)\n",
        "        ax_pred.set_title(f\"Rho: {rho} - Prediction\")\n",
        "        ax_pred.set_xlabel(\"t - Time\")  # Horizontal axis label\n",
        "        ax_pred.set_ylabel(\"u(t) - Value\")  # Vertical axis label\n",
        "        ax_pred.legend()\n",
        "\n",
        "\n",
        "        # Plot absolute and percentage errors (Row 2)\n",
        "        ax_err = axes[1, col]\n",
        "        #ax_err.plot(abs_error, label=\"Absolute Error\", color=\"red\", linewidth=2)\n",
        "        ax_err.plot(percentage_error, label=\"Percentage Error\", color=\"green\", linestyle=\"dotted\", linewidth=2)\n",
        "        ax_err.set_title(f\"Rho: {rho} - Relative Error (%)\")\n",
        "        ax_err.set_xlabel(\"t - Time\")  # Horizontal axis label\n",
        "        ax_err.set_ylabel(\"delta u(t) (%)\")  # Vertical axis label\n",
        "        ax_err.legend()\n",
        "\n",
        "    # Hide unused subplots if num_rho < num_cols\n",
        "    for idx in range(num_rho, num_cols):\n",
        "        axes[0, idx].axis(\"off\")\n",
        "        axes[1, idx].axis(\"off\")\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure if a save path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300)\n",
        "        print(f\"Figure saved to {save_path}\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEyKNcYPEUvy"
      },
      "outputs": [],
      "source": [
        "def test_on_parameter_range(rho_values, dataset, model):\n",
        "    predictions = {}\n",
        "    analytical_solutions = {}\n",
        "    errors = {}\n",
        "\n",
        "    for rho in rho_values:\n",
        "        # Get test points for the current rho (with time sequencing)\n",
        "        x_test, t_test, _ = dataset.get_test_points(rho)\n",
        "\n",
        "        # Create appropriate rho tensor that matches the shape of x_test\n",
        "        if dataset.use_time_sequencing:\n",
        "            # For time-sequenced data, rho tensor needs to match the time sequence dimension\n",
        "            rho_test = torch.full((x_test.shape[0], x_test.shape[1], 1), rho, device=device)\n",
        "        else:\n",
        "            # Original approach for non-time-sequenced data\n",
        "            rho_test = torch.full_like(x_test, rho).to(device)\n",
        "\n",
        "        # Compute model predictions\n",
        "        with torch.no_grad():\n",
        "            pred = model(x_test, t_test, rho_test)\n",
        "\n",
        "            # For time-sequenced data, we need to handle the shape properly\n",
        "            if dataset.use_time_sequencing:\n",
        "                # Extract the relevant output dimension (first output channel)\n",
        "                # Shape will be (batch_size, time_steps, 1)\n",
        "                if pred.dim() > 3:\n",
        "                    pred = pred[:, :, 0:1]\n",
        "\n",
        "                # We'll use just the first time step for visualization\n",
        "                # This simplifies comparison but you could modify to use all time steps\n",
        "                pred_for_viz = pred[:, 0].cpu().detach().numpy().reshape(-1)\n",
        "            else:\n",
        "                # Original approach for non-time-sequenced data\n",
        "                pred_for_viz = pred[:, 0:1].cpu().detach().numpy().reshape(-1)\n",
        "\n",
        "        # Compute analytical solution\n",
        "        with torch.no_grad():\n",
        "            if dataset.use_time_sequencing:\n",
        "                # For time-sequenced data, compute analytical solution for each time step\n",
        "                # But use just the first time step for visualization\n",
        "                u_analytical = dataset.analytical_solution(x_test[:, 0:1], t_test[:, 0:1], rho)\n",
        "                u_analytical_for_viz = u_analytical.cpu().detach().numpy().reshape(-1)\n",
        "            else:\n",
        "                # Original approach for non-time-sequenced data\n",
        "                u_analytical_for_viz = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(-1)\n",
        "\n",
        "        # Compute error (using the visualization data)\n",
        "        error = np.abs(u_analytical_for_viz - pred_for_viz)\n",
        "\n",
        "        # Store results for visualization\n",
        "        predictions[rho] = pred_for_viz\n",
        "        analytical_solutions[rho] = u_analytical_for_viz\n",
        "        errors[rho] = error\n",
        "\n",
        "    return predictions, analytical_solutions, errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD9MqinwEUvy"
      },
      "outputs": [],
      "source": [
        "def perform_grid_search(config, dataset, rho_values, device, models, normalize_res_values, normalize_ic_values, adaptive_loss, alpha_values, epsilon_values, iteration_steps):\n",
        "    \"\"\"\n",
        "    Perform a grid search over model and hyperparameter variations to find the best model.\n",
        "\n",
        "    Args:\n",
        "        config (HyperparamConfig): Configuration object containing hyperparameters.\n",
        "        dataset (ODEData): Dataset object for training and evaluation.\n",
        "        rho_values (list): List of rho values for training and evaluation.\n",
        "        device (torch.device): Device to run the training on (e.g., 'cuda:0').\n",
        "        models (list): List of model variations (tuples of model and hyperparameters).\n",
        "        normalize_res_values (list): List of boolean values for normalizing residuals.\n",
        "        normalize_ic_values (list): List of boolean values for normalizing initial conditions.\n",
        "        adaptive_loss (list): List of boolean values for enabling adaptive loss weighting.\n",
        "        adaptive_loss_weighting_coeffs (list): List of coefficients for adaptive loss weighting.\n",
        "        alpha_values (list): List of alpha values for normalization.\n",
        "        epsilon_values (list): List of epsilon values for normalization.\n",
        "        iteration_steps (list): List of iteration steps for training.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_model, best_config, best_l1, best_l2)\n",
        "            - best_model: The model with the best performance.\n",
        "            - best_config: The configuration of the best model.\n",
        "            - best_l1: The lowest average L1 error achieved.\n",
        "            - best_l2: The lowest average L2 error achieved.\n",
        "    \"\"\"\n",
        "    best_model = None\n",
        "    best_config = None\n",
        "    best_l1 = float('inf')\n",
        "    best_l2 = float('inf')\n",
        "\n",
        "    for model, emb_dim, num_heads, hidden_dim,num_layer in models:\n",
        "        for normalize_res in normalize_res_values:\n",
        "            for normalize_ic in normalize_ic_values:\n",
        "                for ad_loss,a_coeff in adaptive_loss:\n",
        "                    for alpha in alpha_values:\n",
        "                        for epsilon in epsilon_values:\n",
        "                            for total_i in iteration_steps:\n",
        "                                # Update the configuration\n",
        "                                config.normalize_res = normalize_res\n",
        "                                config.normalize_ic = normalize_ic\n",
        "                                config.adaptive_loss_coeff = a_coeff\n",
        "                                config.alpha = alpha\n",
        "                                config.epsilon = epsilon\n",
        "                                config.total_i = total_i\n",
        "                                config.adaptive_loss_weighting = ad_loss\n",
        "                                config.num_layer = num_layer\n",
        "                                config.hidden_dim = hidden_dim\n",
        "                                config.emb_dim = emb_dim\n",
        "                                config.num_heads = num_heads\n",
        "                                config.validate()\n",
        "\n",
        "                                # Call the training function\n",
        "                                config_dict = config.to_dict()\n",
        "                                if config.optimizer == \"adam\":\n",
        "                                    optim = Adam(model.parameters(), lr=config_dict[\"learning_rate\"])\n",
        "                                loss_track, avg_l1, avg_l2 = run_training(config_dict, model, optim, dataset, rho_values, device)\n",
        "\n",
        "                                # Update the best model if the current one is better\n",
        "                                if avg_l1 < best_l1 and avg_l2 < best_l2:\n",
        "                                    best_model = model\n",
        "                                    best_config = copy.deepcopy(config)\n",
        "\n",
        "                                    best_l1 = avg_l1\n",
        "                                    best_l2 = avg_l2\n",
        "\n",
        "    return best_model, best_config, best_l1, best_l2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h6yni5YEUvy"
      },
      "source": [
        "### Hyperparameter tuning through grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlIdfv9hEUvy"
      },
      "source": [
        "##### Tuning step 1\n",
        "\n",
        "We explore the impact of the following params on convergence and performance:\n",
        "1. adaptive_loss - on/off\n",
        "2. different hidden sizes\n",
        "3. different head counts\n",
        "\n",
        "Other params remain unchanged:\n",
        "- `iteration_steps` - **fixed value**!\n",
        "- `embedding_dim` - **fixed value**!\n",
        "- `num_layers` - **fixed value**!\n",
        "\n",
        "**Also:** Here we use a low parameter regime (0.5-1) to train the model. This is not a representative range, but still interesting to explore. See `1d_logistic_ode_regimes.ipynb` for further information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "kPItyNxBEUvy",
        "outputId": "aec54f0b-50c8-43e1-e03f-6d1f9f376910"
      },
      "outputs": [],
      "source": [
        "# Define rho values\n",
        "rho_values = [0.5, 0.7, 0.8, 1.0]\n",
        "\n",
        "# Create the dataset\n",
        "dataset = ODEData(t_range=[0, 1], rho_values=rho_values, t_points=101, constant_x=1.0, device='cuda:0', use_time_sequencing=True)\n",
        "\n",
        "# Define variations\n",
        "normalize_res_values = [False]  # [True, False]\n",
        "normalize_ic_values = [False]  # [True, False]\n",
        "adaptive_loss = [(True,0.9), (False,None)]\n",
        "#adaptive_loss_weighting_coeffs = [0.9]#[0.7, 0.9]\n",
        "alpha_values = [None]  # [0.1, 0.25]\n",
        "epsilon_values = [None]  # [0.1, 0.3]\n",
        "iteration_steps = [50] #[300, 500]  # [50, 100, 400]\n",
        "\n",
        "# Define model variations\n",
        "models = []\n",
        "model_variations = [\n",
        "    (32, 1, 64, 1),\n",
        "    (32, 2, 64, 1),\n",
        "    (32, 4, 64, 1),\n",
        "    (32, 1, 128, 1),\n",
        "    (32, 2, 128, 1),\n",
        "    (32, 4, 128, 1),\n",
        "    (32, 1, 256, 1),\n",
        "    (32, 2, 256, 1),\n",
        "    (32, 4, 256, 1),\n",
        "    (32, 1, 512, 1),\n",
        "]\n",
        "for emb_dim, num_heads, hidden_dim, num_layer in model_variations:\n",
        "    model = PINNsformer(\n",
        "        d_out=config.out_dim,\n",
        "        d_model=emb_dim,\n",
        "        d_hidden=hidden_dim,\n",
        "        N=num_layer,\n",
        "        heads=num_heads\n",
        "    ).to(device)\n",
        "    model.apply(init_weights)\n",
        "    models.append((model, emb_dim, num_heads, hidden_dim, num_layer))\n",
        "\n",
        "# Perform grid search\n",
        "best_model1, best_config1, best_l11, best_l21 = perform_grid_search(\n",
        "    config=config,\n",
        "    dataset=dataset,\n",
        "    rho_values=rho_values,\n",
        "    device='cuda:0',\n",
        "    models=models,\n",
        "    normalize_res_values=normalize_res_values,\n",
        "    normalize_ic_values=normalize_ic_values,\n",
        "    adaptive_loss=adaptive_loss,\n",
        "    alpha_values=alpha_values,\n",
        "    epsilon_values=epsilon_values,\n",
        "    iteration_steps=iteration_steps\n",
        ")\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Model Configuration:\")\n",
        "print(best_config1)\n",
        "print(f\"Best Average L1 Error: {best_l11}\")\n",
        "print(f\"Best Average L2 Error: {best_l21}\")\n",
        "# Save the best model\n",
        "model_path = weights_dir / \"1d_logistic_ode_low_regime_pinnsformer_v1.pt\"  # Path to save the model\n",
        "torch.save(best_model1.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "V8yEOuevEUvz",
        "outputId": "5e60832e-d6f7-41e7-eb48-c1428489d393"
      },
      "outputs": [],
      "source": [
        "best_model1.eval()\n",
        "predictions, analytical_solutions, errors = test_on_parameter_range(rho_values, dataset, best_model1)\n",
        "\n",
        "# Plot predictions and errors\n",
        "plot_predictions_and_errors(\n",
        "    rho_values=rho_values,\n",
        "    predictions=predictions,\n",
        "    analytical_solutions=analytical_solutions,\n",
        "    errors=errors,\n",
        "    num_cols=len(rho_values),\n",
        "    save_path=images_dir / \"predictions_and_errors_pinnsformer_v1.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGRdHVCqEUvz"
      },
      "source": [
        "##### Tuning step 2\n",
        "The previous results show, that higher `hidden_dim` values\n",
        "are in fact not helpful. But increasing `head_num` is useful.\n",
        "\n",
        "\n",
        "We explore the impact of the following params on convergence and performance:\n",
        "1. adaptive_loss - on/off\n",
        "2. only small hidden sizes (64,128)\n",
        "3. different head counts\n",
        "4. increasing number of iteration steps\n",
        "\n",
        "Other params remain unchanged:\n",
        "- `embedding_dim` - **fixed value**!\n",
        "- `num_layers` - **fixed value**!\n",
        "\n",
        "**Also:** Here we use a low parameter regime (0.5-1) to train the model. This is not a representative range, but still interesting to explore. See `1d_logistic_ode_regimes.ipynb` for further information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "Q3Pb6ju_EUvz",
        "outputId": "6cf22324-9198-4674-a3d6-be7857046b01"
      },
      "outputs": [],
      "source": [
        "# Define rho values\n",
        "rho_values = [0.5, 0.7, 0.8, 1.0]  # Small range\n",
        "\n",
        "# Create the dataset\n",
        "dataset = ODEData(t_range=[0, 1], rho_values=rho_values, t_points=101, constant_x=1.0, device='cuda:0', use_time_sequencing=True)\n",
        "\n",
        "# Define variations\n",
        "normalize_res_values = [False]  # [True, False]\n",
        "normalize_ic_values = [False]  # [True, False]\n",
        "adaptive_loss = [(False,None)] #[(True,0.9), (False,None)]\n",
        "#adaptive_loss_weighting_coeffs = [0.9]#[0.7, 0.9]\n",
        "alpha_values = [None]  # [0.1, 0.25]\n",
        "epsilon_values = [None]  # [0.1, 0.3]\n",
        "iteration_steps = [120] #[200, 300, 500]  # [50, 100, 400]\n",
        "\n",
        "# Define model variations\n",
        "models = []\n",
        "\"\"\"\n",
        "model_variations = [\n",
        "    (32, 1, 64, 1),\n",
        "    (32, 2, 64, 1),\n",
        "    (32, 2, 128, 1),\n",
        "    (32, 1, 128, 1),\n",
        "]\n",
        "\"\"\"\n",
        "model_variations = [\n",
        "    (32, 2, 128, 1),\n",
        "    (32, 4, 128, 1),\n",
        "    (32, 2, 256, 1),\n",
        "    (32, 4, 256, 1),\n",
        "]\n",
        "for emb_dim, num_heads, hidden_dim, num_layer in model_variations:\n",
        "    model = PINNsformer(\n",
        "        d_out=config.out_dim,\n",
        "        d_model=emb_dim,\n",
        "        d_hidden=hidden_dim,\n",
        "        N=num_layer,\n",
        "        heads=num_heads\n",
        "    ).to(device)\n",
        "    model.apply(init_weights)\n",
        "    models.append((model, emb_dim, num_heads, hidden_dim, num_layer))\n",
        "\n",
        "# Perform grid search\n",
        "best_model2, best_config2, best_l12, best_l22 = perform_grid_search(\n",
        "    config=config,\n",
        "    dataset=dataset,\n",
        "    rho_values=rho_values,\n",
        "    device='cuda:0',\n",
        "    models=models,\n",
        "    normalize_res_values=normalize_res_values,\n",
        "    normalize_ic_values=normalize_ic_values,\n",
        "    adaptive_loss=adaptive_loss,\n",
        "    alpha_values=alpha_values,\n",
        "    epsilon_values=epsilon_values,\n",
        "    iteration_steps=iteration_steps\n",
        ")\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Model Configuration:\")\n",
        "print(best_config2)\n",
        "print(f\"Best Average L1 Error: {best_l12}\")\n",
        "print(f\"Best Average L2 Error: {best_l22}\")\n",
        "# Save the best model\n",
        "model_path = weights_dir / \"1d_logistic_ode_low_regime_pinnsformer_v2.pt\"  # Path to save the model\n",
        "torch.save(best_model2.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "MT9XKRkJK8wE",
        "outputId": "f1cf259a-4785-4bc5-d7fe-901866b9df4b"
      },
      "outputs": [],
      "source": [
        "best_model2.eval()\n",
        "predictions, analytical_solutions, errors = test_on_parameter_range(rho_values, dataset, best_model2)\n",
        "\n",
        "# Plot predictions and errors\n",
        "plot_predictions_and_errors(\n",
        "    rho_values=rho_values,\n",
        "    predictions=predictions,\n",
        "    analytical_solutions=analytical_solutions,\n",
        "    errors=errors,\n",
        "    num_cols=len(rho_values),\n",
        "    save_path=images_dir / \"predictions_and_errors_pinnsformer_v2.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiXSc4CH9qhv"
      },
      "source": [
        "##### Tuning step 3\n",
        "The previous results show, that higher `hidden_dim` values\n",
        "are in fact not helpful. But increasing `head_num` is useful.\n",
        "\n",
        "\n",
        "We explore the impact of the following params on convergence and performance:\n",
        "1. adaptive_loss - on/off\n",
        "2. only small hidden sizes (64,128)\n",
        "3. different head counts\n",
        "4. increasing number of iteration steps\n",
        "\n",
        "Other params remain unchanged:\n",
        "- `embedding_dim` - **fixed value**!\n",
        "- `num_layers` - **fixed value**!\n",
        "\n",
        "**Also:** Here we use a low parameter regime (0.5-1) to train the model. This is not a representative range, but still interesting to explore. See `1d_logistic_ode_regimes.ipynb` for further information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "0wWUlRFA9qhv",
        "outputId": "80faf1f1-e950-40c8-f7a3-4ff9cf063d22"
      },
      "outputs": [],
      "source": [
        "# Define rho values\n",
        "rho_values = [0.5, 0.7, 0.8, 1.0]  # Small range\n",
        "\n",
        "# Create the dataset\n",
        "dataset = ODEData(t_range=[0, 1], rho_values=rho_values, t_points=101, constant_x=1.0, device='cuda:0', use_time_sequencing=True)\n",
        "\n",
        "# Define variations\n",
        "normalize_res_values = [False]  # [True, False]\n",
        "normalize_ic_values = [False]  # [True, False]\n",
        "adaptive_loss = [(False,None)] #[(True,0.9), (False,None)]\n",
        "#adaptive_loss_weighting_coeffs = [0.9]#[0.7, 0.9]\n",
        "alpha_values = [None]  # [0.1, 0.25]\n",
        "epsilon_values = [None]  # [0.1, 0.3]\n",
        "iteration_steps = [250] #[200, 300, 500]  # [50, 100, 400]\n",
        "\n",
        "# Define model variations\n",
        "models = []\n",
        "\"\"\"\n",
        "model_variations = [\n",
        "    (32, 1, 64, 1),\n",
        "    (32, 2, 64, 1),\n",
        "    (32, 2, 128, 1),\n",
        "    (32, 1, 128, 1),\n",
        "]\n",
        "\"\"\"\n",
        "model_variations = [\n",
        "    (32, 2, 256, 1),\n",
        "    (32, 4, 256, 1),\n",
        "]\n",
        "for emb_dim, num_heads, hidden_dim, num_layer in model_variations:\n",
        "    model = PINNsformer(\n",
        "        d_out=config.out_dim,\n",
        "        d_model=emb_dim,\n",
        "        d_hidden=hidden_dim,\n",
        "        N=num_layer,\n",
        "        heads=num_heads\n",
        "    ).to(device)\n",
        "    model.apply(init_weights)\n",
        "    models.append((model, emb_dim, num_heads, hidden_dim, num_layer))\n",
        "\n",
        "# Perform grid search\n",
        "best_model3, best_config3, best_l13, best_l23 = perform_grid_search(\n",
        "    config=config,\n",
        "    dataset=dataset,\n",
        "    rho_values=rho_values,\n",
        "    device='cuda:0',\n",
        "    models=models,\n",
        "    normalize_res_values=normalize_res_values,\n",
        "    normalize_ic_values=normalize_ic_values,\n",
        "    adaptive_loss=adaptive_loss,\n",
        "    alpha_values=alpha_values,\n",
        "    epsilon_values=epsilon_values,\n",
        "    iteration_steps=iteration_steps\n",
        ")\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Model Configuration:\")\n",
        "print(best_config3)\n",
        "print(f\"Best Average L1 Error: {best_l13}\")\n",
        "print(f\"Best Average L2 Error: {best_l23}\")\n",
        "# Save the best model\n",
        "model_path = weights_dir / \"1d_logistic_ode_low_regime_pinnsformer_v3.pt\"  # Path to save the model\n",
        "torch.save(best_model3.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "5sKOuvFL9qhw",
        "outputId": "00f4a952-df87-4105-c0a6-d690292cccf0"
      },
      "outputs": [],
      "source": [
        "best_model3.eval()\n",
        "predictions, analytical_solutions, errors = test_on_parameter_range(rho_values, dataset, best_model3)\n",
        "\n",
        "# Plot predictions and errors\n",
        "plot_predictions_and_errors(\n",
        "    rho_values=rho_values,\n",
        "    predictions=predictions,\n",
        "    analytical_solutions=analytical_solutions,\n",
        "    errors=errors,\n",
        "    num_cols=len(rho_values),\n",
        "    save_path=images_dir / \"predictions_and_errors_pinnsformer_v3.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGFq3sDAEUvz"
      },
      "source": [
        "### Testing interpolation and extrapolation capabilities of models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ICcIXVNEUvz"
      },
      "outputs": [],
      "source": [
        "## add additional import for visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjd_-PgFEUvz"
      },
      "source": [
        "**Best model configuration:**\n",
        "\n",
        "{'total_i': 400, 'dataset': '1d-logistic-ode', 'model_name': 'pinn_ff', 'in_dim': 3, 'hidden_dim': 256, 'out_dim': 1, 'num_layer': 4, 'init_weights': 'xavier uniform', 'bias_fill': 0.01, 'init_activ_func': 'tanh', 'subseq_activ_func': 'gelu', 'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 128, 'normalize_res': False, 'normalize_ic': False, 'adaptive_loss_weighting': True, 'adaptive_loss_coeff': 0.9}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAc0yUi69qhx"
      },
      "outputs": [],
      "source": [
        "best_model = best_model2\n",
        "best_config = best_config2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9uAddZ6EUv0"
      },
      "outputs": [],
      "source": [
        "load_existing_model = False\n",
        "\n",
        "if load_existing_model == True:\n",
        "    best_model = PINNsformer(\n",
        "                    d_out=best_config.out_dim,\n",
        "                    d_model=best_config.emb_dim,\n",
        "                    d_hidden=best_config.hidden_dim,\n",
        "                    N=best_config.num_layer,\n",
        "                    heads=best_config.num_heads).to(device)\n",
        "\n",
        "    model_path = weights_dir / \"1d_logistic_ode_fls_extended.pt\"  # Path to the saved model\n",
        "    best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    best_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6-OmcrcEUv0"
      },
      "outputs": [],
      "source": [
        "# Function 1: Generate uniformly spaced  values in a given range\n",
        "def generate_rho_values(start, end, num=10):\n",
        "    return np.linspace(start, end, num).tolist()\n",
        "\n",
        "# Function 2: Evaluate model on  values and return error stats\n",
        "def evaluate_rho_range(rho_values, dataset, model, device='cpu'):\n",
        "    \"\"\"\n",
        "    Evaluate model performance across a range of rho values, using time-sequenced data.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "\n",
        "    for rho in rho_values:\n",
        "        # Generate test points with time sequencing enabled\n",
        "        x_test, t_test, _ = dataset.get_interior_input_without_points()\n",
        "\n",
        "        # Create appropriate rho tensor that matches the time-sequenced shape\n",
        "        # For time-sequenced data, shape will be (batch_size, time_steps, 1)\n",
        "        rho_test = torch.full((x_test.shape[0], x_test.shape[1], 1), rho).to(device)\n",
        "\n",
        "        # Make predictions with time-sequenced data\n",
        "        with torch.no_grad():\n",
        "            pred = model(x_test.to(device), t_test.to(device), rho_test)\n",
        "\n",
        "            # Ensure the prediction has the expected shape with time steps\n",
        "            # It should be (batch_size, time_steps, 1) or similar\n",
        "            if pred.dim() < 3:\n",
        "                # If model doesn't preserve time dimension, reshape\n",
        "                pred = pred.unsqueeze(1).expand(-1, x_test.shape[1], -1)\n",
        "\n",
        "            # Flatten the prediction for error calculation\n",
        "            # This combines all time steps into a single array\n",
        "            pred_flattened = pred.cpu().detach().numpy().reshape(-1)\n",
        "\n",
        "        # Compute analytical solution for each time step\n",
        "        with torch.no_grad():\n",
        "            u_analytical = dataset.analytical_solution(\n",
        "                x_test, t_test, rho\n",
        "            ).cpu().detach().numpy()\n",
        "\n",
        "            # Flatten the analytical solution to match prediction\n",
        "            u_analytical_flattened = u_analytical.reshape(-1)\n",
        "\n",
        "        # for debugging\n",
        "        # print(f\"Prediction shape: {pred_flattened.shape}, Analytical shape: {u_analytical_flattened.shape}\")\n",
        "\n",
        "        # Calculate relative errors using flattened arrays\n",
        "        rl1 = np.sum(np.abs(u_analytical_flattened - pred_flattened)) / np.sum(np.abs(u_analytical_flattened))\n",
        "        rl2 = np.sqrt(np.sum((u_analytical_flattened - pred_flattened) ** 2) / np.sum(u_analytical_flattened ** 2))\n",
        "\n",
        "        results.append({'rho': rho, 'rl1': rl1, 'rl2': rl2})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Function 3: Plot scatter and compute Pearson correlation\n",
        "def visualize_error_vs_rho(df, range_label=\"\"):\n",
        "    sns.set_theme(style='whitegrid')\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    axs[0].scatter(df['rho'], df['rl1'], color='blue', label='L1 Error')\n",
        "    axs[0].set_title(f'L1 Error vs  ({range_label})')\n",
        "    axs[0].set_xlabel('')\n",
        "    axs[0].set_ylabel('Relative L1 Error')\n",
        "\n",
        "    axs[1].scatter(df['rho'], df['rl2'], color='green', label='L2 Error')\n",
        "    axs[1].set_title(f'L2 Error vs  ({range_label})')\n",
        "    axs[1].set_xlabel('')\n",
        "    axs[1].set_ylabel('Relative L2 Error')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Pearson correlations\n",
        "    p_l1, _ = pearsonr(df['rho'], df['rl1'])\n",
        "    p_l2, _ = pearsonr(df['rho'], df['rl2'])\n",
        "\n",
        "    print(f\"Pearson correlation ( vs L1): {p_l1:.3f}\")\n",
        "    print(f\"Pearson correlation ( vs L2): {p_l2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jB_by6ATEUv0",
        "outputId": "c17a6afa-a897-49e7-a98e-7e705ff2eb7d"
      },
      "outputs": [],
      "source": [
        "# Example usage loop over different rho ranges\n",
        "ranges = [\n",
        "    (0.5, 1.0),   # Low variance\n",
        "    (0.5, 4.0),   # Medium variance\n",
        "    (0.5, 10.0),  # High variance\n",
        "]\n",
        "dataset = ODEData(t_range=[0, 1], rho_values=[0.1, 0.3, 0.4], t_points=101, constant_x=1.0, device='cuda:0', use_time_sequencing=True) # here we can use dummy rho values\n",
        "\n",
        "for start, end in ranges:\n",
        "    rho_vals = generate_rho_values(start, end, num=10)\n",
        "    df_results = evaluate_rho_range(rho_vals, dataset, best_model, device='cuda:0')\n",
        "    visualize_error_vs_rho(df_results, range_label=f\"{start}{end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ItfMtiUEUv0"
      },
      "outputs": [],
      "source": [
        "def compute_average_error_per_range(ranges, dataset, model, device='cuda:0', points_per_range=10):\n",
        "    avg_errors = []\n",
        "\n",
        "    for start, end in ranges:\n",
        "        rho_values = np.linspace(start, end, points_per_range)\n",
        "        df = evaluate_rho_range(rho_values, dataset, model, device=device)\n",
        "\n",
        "        mean_l1 = df['rl1'].mean()\n",
        "        mean_l2 = df['rl2'].mean()\n",
        "        avg_errors.append({\n",
        "            'range': f\"{start:.1f}{end:.1f}\",\n",
        "            'mean_l1': mean_l1,\n",
        "            'mean_l2': mean_l2\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(avg_errors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ8cwoGcEUv1"
      },
      "outputs": [],
      "source": [
        "def plot_average_errors(df_avg):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot L1 Error\n",
        "    sns.barplot(data=df_avg, x='range', y='mean_l1', ax=axs[0], hue='range', palette=\"Blues_d\", dodge=False)\n",
        "    axs[0].set_title('Average L1 Error per  Range')\n",
        "    axs[0].set_ylabel('Mean L1 Error')\n",
        "    axs[0].set_xlabel(' Range')\n",
        "    legend = axs[0].get_legend()\n",
        "    if legend:  # Check if the legend exists\n",
        "        legend.remove()\n",
        "\n",
        "    # Plot L2 Error\n",
        "    sns.barplot(data=df_avg, x='range', y='mean_l2', ax=axs[1], hue='range', palette=\"Greens_d\", dodge=False)\n",
        "    axs[1].set_title('Average L2 Error per  Range')\n",
        "    axs[1].set_ylabel('Mean L2 Error')\n",
        "    axs[1].set_xlabel(' Range')\n",
        "    legend = axs[1].get_legend()\n",
        "    if legend:  # Check if the legend exists\n",
        "        legend.remove()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "r1TPKxZ0EUv1",
        "outputId": "80762250-4182-4284-b296-b80571693d9a"
      },
      "outputs": [],
      "source": [
        "ranges = [\n",
        "    (-2.0, 1.0),\n",
        "    (0.5, 1.0),\n",
        "    (0.5, 4.0),\n",
        "    (0.5, 10.0),\n",
        "    (-5.0, 20.0)\n",
        "]\n",
        "\n",
        "df_avg = compute_average_error_per_range(ranges, dataset, best_model, device='cuda:0')\n",
        "plot_average_errors(df_avg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-cZkapQ9qh1"
      },
      "source": [
        "### Analyzing convergence speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2DPLvmj9qh1"
      },
      "outputs": [],
      "source": [
        "# imports that might be missing\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "load_existing_model = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvLRFSFy9qh1"
      },
      "outputs": [],
      "source": [
        "if load_existing_model == True:\n",
        "    best_model = PINNsformer(\n",
        "                    d_out=best_config.out_dim,\n",
        "                    d_model=best_config.emb_dim,\n",
        "                    d_hidden=best_config.hidden_dim,\n",
        "                    N=best_config.num_layer,\n",
        "                    heads=best_config.num_heads).to(device)\n",
        "\n",
        "    model_path = weights_dir / \"1d_logistic_ode_low_regime_pinnsformer_v2.pt\"  # Path to the saved model\n",
        "    best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    #best_model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC9bUZcV9qh1"
      },
      "outputs": [],
      "source": [
        "def run_fine_tuning_experiment(target_rho, best_model, iterations=50, device='cuda:0'):\n",
        "    \"\"\"\n",
        "    Run a fine-tuning experiment comparing from-scratch training vs fine-tuning\n",
        "    for a specific target rho value.\n",
        "\n",
        "    Args:\n",
        "        target_rho (float): The target rho value to fine-tune on\n",
        "        best_model (nn.Module): The pre-trained model to fine-tune\n",
        "        iterations (int): Number of training iterations\n",
        "        device (str): Device to run training on\n",
        "\n",
        "    Returns:\n",
        "        dict: Contains baseline_metrics and finetune_metrics\n",
        "    \"\"\"\n",
        "\n",
        "    from_scratch_model = copy.deepcopy(best_model)\n",
        "    from_scratch_model.apply(init_weights)\n",
        "    finetuned_model = copy.deepcopy(best_model)\n",
        "\n",
        "    from_scratch_optimizer = torch.optim.Adam(from_scratch_model.parameters(), lr=1e-3)\n",
        "    finetune_optimizer = torch.optim.Adam(finetuned_model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "    dataset = ODEData(\n",
        "        t_range=[0, 1],\n",
        "        rho_values=[target_rho],\n",
        "        t_points=101,\n",
        "        constant_x=1.0,\n",
        "        device=device,\n",
        "        use_time_sequencing=True\n",
        "    )\n",
        "\n",
        "    # Configure training settings\n",
        "    model_config_dict = {\n",
        "        \"total_i\": iterations,\n",
        "        \"batch_size\": 32,\n",
        "        \"normalize_res\": False,\n",
        "        \"normalize_ic\": False,\n",
        "        \"adaptive_loss_weighting\": False,\n",
        "        \"adaptive_loss_coeff\": 0.9,\n",
        "        \"alpha\": 1.0,\n",
        "        \"epsilon\": 1e-8\n",
        "    }\n",
        "\n",
        "    # Tracking structures for comparing convergence\n",
        "    baseline_metrics = {'iterations': [], 'loss': [], 'rl1': [], 'rl2': []}\n",
        "    finetune_metrics = {'iterations': [], 'loss': [], 'rl1': [], 'rl2': []}\n",
        "\n",
        "    model_config_dict[\"mode\"] = \"from_scratch\"\n",
        "    print(f\"Training baseline model from scratch for ={target_rho}...\")\n",
        "    baseline_loss_track, baseline_avg_l1, baseline_avg_l2 = run_training(\n",
        "        model_config_dict,\n",
        "        from_scratch_model,\n",
        "        from_scratch_optimizer,\n",
        "        dataset,\n",
        "        [target_rho],\n",
        "        device\n",
        "    )\n",
        "\n",
        "    model_config_dict[\"mode\"] = \"fine_tuned\"\n",
        "    print(f\"Fine-tuning pre-trained model for ={target_rho}...\")\n",
        "    finetune_loss_track, finetune_avg_l1, finetune_avg_l2 = run_training(\n",
        "        model_config_dict,\n",
        "        finetuned_model,\n",
        "        finetune_optimizer,\n",
        "        dataset,\n",
        "        [target_rho],\n",
        "        device\n",
        "    )\n",
        "\n",
        "    # Extract metrics for plotting\n",
        "    for i, metrics in baseline_loss_track.items():\n",
        "        baseline_metrics['iterations'].append(i)\n",
        "        baseline_metrics['loss'].append(metrics['avg_total_loss'])\n",
        "        baseline_metrics['rl1'].append(metrics[f\"{target_rho}_rl1\"])\n",
        "        baseline_metrics['rl2'].append(metrics[f\"{target_rho}_rl2\"])\n",
        "\n",
        "    for i, metrics in finetune_loss_track.items():\n",
        "        finetune_metrics['iterations'].append(i)\n",
        "        finetune_metrics['loss'].append(metrics['avg_total_loss'])\n",
        "        finetune_metrics['rl1'].append(metrics[f\"{target_rho}_rl1\"])\n",
        "        finetune_metrics['rl2'].append(metrics[f\"{target_rho}_rl2\"])\n",
        "\n",
        "    return {\n",
        "        'target_rho': target_rho,\n",
        "        'baseline_metrics': baseline_metrics,\n",
        "        'finetune_metrics': finetune_metrics,\n",
        "        'baseline_l1': baseline_avg_l1,\n",
        "        'baseline_l2': baseline_avg_l2,\n",
        "        'finetune_l1': finetune_avg_l1,\n",
        "        'finetune_l2': finetune_avg_l2\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEksvkJW9qh2"
      },
      "outputs": [],
      "source": [
        "def plot_convergence_multi_rho(experiment_results, metrics=['rl2', 'loss']):\n",
        "    \"\"\"\n",
        "    Plot convergence comparisons for multiple rho values.\n",
        "\n",
        "    Args:\n",
        "        experiment_results (list): List of dictionaries with experiment results\n",
        "        metrics (list): List of metrics to plot (default: ['rl2', 'loss'])\n",
        "\n",
        "    Returns:\n",
        "        matplotlib.figure.Figure: The generated figure with subplots\n",
        "    \"\"\"\n",
        "    num_rhos = len(experiment_results)\n",
        "    num_metrics = len(metrics)\n",
        "\n",
        "    fig, axes = plt.subplots(num_metrics, num_rhos, figsize=(5*num_rhos, 4*num_metrics))\n",
        "    if num_metrics == 1 and num_rhos == 1:\n",
        "        axes = np.array([[axes]])\n",
        "    elif num_metrics == 1:\n",
        "        axes = np.array([axes])\n",
        "    elif num_rhos == 1:\n",
        "        axes = np.array([[ax] for ax in axes])\n",
        "\n",
        "    for metric_idx, metric in enumerate(metrics):\n",
        "        for rho_idx, result in enumerate(experiment_results):\n",
        "            ax = axes[metric_idx, rho_idx]\n",
        "            target_rho = result['target_rho']\n",
        "            baseline_metrics = result['baseline_metrics']\n",
        "            finetune_metrics = result['finetune_metrics']\n",
        "            ax.plot(\n",
        "                baseline_metrics['iterations'],\n",
        "                baseline_metrics[metric],\n",
        "                'b-o',\n",
        "                label=f'From Scratch'\n",
        "            )\n",
        "            ax.plot(\n",
        "                finetune_metrics['iterations'],\n",
        "                finetune_metrics[metric],\n",
        "                'r-o',\n",
        "                label=f'Fine-tuned'\n",
        "            )\n",
        "\n",
        "            ax.set_xlabel('Iterations')\n",
        "            ax.set_ylabel(f'{metric.upper()}')\n",
        "            ax.set_title(f'={target_rho}')\n",
        "            ax.grid(True, linestyle='--', alpha=0.7)\n",
        "            ax.set_yscale('log')  # Log scale better shows convergence differences\n",
        "\n",
        "            if rho_idx == 0:\n",
        "                ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def calculate_all_convergence_metrics(experiment_results, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Calculate convergence metrics for all experiments.\n",
        "\n",
        "    Args:\n",
        "        experiment_results (list): List of dictionaries with experiment results\n",
        "        threshold (float): Error threshold for determining convergence\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with convergence metrics\n",
        "    \"\"\"\n",
        "    all_comparisons = []\n",
        "\n",
        "    for result in experiment_results:\n",
        "        target_rho = result['target_rho']\n",
        "        baseline_metrics = result['baseline_metrics']\n",
        "        finetune_metrics = result['finetune_metrics']\n",
        "\n",
        "        # Find iterations where models first reach error threshold\n",
        "        try:\n",
        "            baseline_converged_at = next(i for i, v in enumerate(baseline_metrics['rl2'])\n",
        "                                        if v < threshold)\n",
        "        except StopIteration:\n",
        "            baseline_converged_at = \"Did not converge\"\n",
        "\n",
        "        try:\n",
        "            finetune_converged_at = next(i for i, v in enumerate(finetune_metrics['rl2'])\n",
        "                                        if v < threshold)\n",
        "        except StopIteration:\n",
        "            finetune_converged_at = \"Did not converge\"\n",
        "\n",
        "        if isinstance(baseline_converged_at, int) and isinstance(finetune_converged_at, int):\n",
        "            speedup = (baseline_converged_at / finetune_converged_at) if finetune_converged_at > 0 else float('inf')\n",
        "            speedup_percentage = ((baseline_converged_at - finetune_converged_at) / baseline_converged_at) * 100\n",
        "        else:\n",
        "            speedup = \"N/A\"\n",
        "            speedup_percentage = \"N/A\"\n",
        "\n",
        "        all_comparisons.append({\n",
        "            'Rho': target_rho,\n",
        "            'Model': 'Baseline (From Scratch)',\n",
        "            'Final RL1 Error': baseline_metrics['rl1'][-1],\n",
        "            'Final RL2 Error': baseline_metrics['rl2'][-1],\n",
        "            'Iterations to Converge': baseline_converged_at\n",
        "        })\n",
        "\n",
        "        all_comparisons.append({\n",
        "            'Rho': target_rho,\n",
        "            'Model': 'Fine-tuned',\n",
        "            'Final RL1 Error': finetune_metrics['rl1'][-1],\n",
        "            'Final RL2 Error': finetune_metrics['rl2'][-1],\n",
        "            'Iterations to Converge': finetune_converged_at,\n",
        "            'Speedup Factor': speedup,\n",
        "            'Improvement %': speedup_percentage if isinstance(speedup_percentage, (int, float)) else speedup_percentage\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(all_comparisons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNlvGs0r9qh3"
      },
      "outputs": [],
      "source": [
        "def write_experiment_summary(experiment_results, output_file='fine_tuning_summary.txt', threshold=0.8):\n",
        "    \"\"\"\n",
        "    Write a summary of fine-tuning experiment results to a text file.\n",
        "\n",
        "    Args:\n",
        "        experiment_results (list): List of dictionaries with experiment results\n",
        "        output_file (str or Path): Path to the output summary file\n",
        "        threshold (float): Error threshold for convergence determination\n",
        "    \"\"\"\n",
        "\n",
        "    output_path = Path(output_file)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(output_path, 'w') as summary_file:\n",
        "        summary_file.write(\"=== Summary of Findings ===\\n\\n\")\n",
        "\n",
        "        for result in experiment_results:\n",
        "            target_rho = result['target_rho']\n",
        "            baseline_l2 = result['baseline_metrics']['rl2'][-1]\n",
        "            finetune_l2 = result['finetune_metrics']['rl2'][-1]\n",
        "\n",
        "            # Calculate improvement percentage\n",
        "            improvement = ((baseline_l2 - finetune_l2) / baseline_l2) * 100\n",
        "\n",
        "            # Write to file\n",
        "            summary_file.write(f\"Rho = {target_rho}:\\n\")\n",
        "            summary_file.write(f\"  - From scratch final RL2: {baseline_l2:.6f}\\n\")\n",
        "            summary_file.write(f\"  - Fine-tuned final RL2: {finetune_l2:.6f}\\n\")\n",
        "            summary_file.write(f\"  - Error reduction: {improvement:.2f}%\\n\")\n",
        "\n",
        "            # Try to identify if convergence threshold was reached\n",
        "            try:\n",
        "                baseline_converged_at = next(i for i, v in enumerate(result['baseline_metrics']['rl2']) if v < threshold)\n",
        "                baseline_converged = True\n",
        "            except StopIteration:\n",
        "                baseline_converged = False\n",
        "\n",
        "            try:\n",
        "                finetune_converged_at = next(i for i, v in enumerate(result['finetune_metrics']['rl2']) if v < threshold)\n",
        "                finetune_converged = True\n",
        "            except StopIteration:\n",
        "                finetune_converged = False\n",
        "\n",
        "            if baseline_converged and finetune_converged:\n",
        "                if finetune_converged_at == 0:\n",
        "                    # Handle the case where fine-tuned model converged immediately\n",
        "                    summary_file.write(f\"  - Fine-tuned model already met threshold - immediate convergence!\\n\")\n",
        "                    summary_file.write(f\"  - Baseline model converged in {baseline_converged_at} iterations.\\n\")\n",
        "                elif baseline_converged_at == 0:\n",
        "                    # Handle case where baseline model converged immediately\n",
        "                    summary_file.write(f\"  - Both models converged immediately - they already met the threshold.\\n\")\n",
        "                else:\n",
        "                    # Normal case - both converged after some iterations\n",
        "                    speedup = baseline_converged_at / finetune_converged_at\n",
        "                    summary_file.write(f\"  - Convergence speedup: {speedup:.2f}x (iterations: {finetune_converged_at} vs {baseline_converged_at})\\n\")\n",
        "            elif finetune_converged and not baseline_converged:\n",
        "                summary_file.write(f\"  - Only fine-tuned model converged (in {finetune_converged_at} iterations)\\n\")\n",
        "            elif baseline_converged and not finetune_converged:\n",
        "                summary_file.write(f\"  - Only baseline model converged (in {baseline_converged_at} iterations)\\n\")\n",
        "            else:\n",
        "                summary_file.write(f\"  - Neither model converged to threshold {threshold}\\n\")\n",
        "\n",
        "            summary_file.write(\"\\n\")\n",
        "\n",
        "        # Add timestamp\n",
        "        import datetime\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        summary_file.write(f\"\\nSummary generated on: {timestamp}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "16lSxIRb9qh3",
        "outputId": "29fd55bc-bbe3-43e1-ae6a-2c4695a988d4"
      },
      "outputs": [],
      "source": [
        "# Define target rho values for fine-tuning experiments\n",
        "target_rho_values = [0.5, 1.0, 3.0, 4.0, 5.0, 6.0]\n",
        "fine_tuning_iterations = 60\n",
        "device = 'cuda:0'\n",
        "\n",
        "base_dir = Path(\".\") / \"low_regime\"\n",
        "convergence_dir = base_dir / \"convergence\"\n",
        "convergence_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "experiment_results = []\n",
        "for target_rho in target_rho_values:\n",
        "    result = run_fine_tuning_experiment(\n",
        "        target_rho=target_rho,\n",
        "        best_model=best_model,\n",
        "        iterations=fine_tuning_iterations,\n",
        "        device=device\n",
        "    )\n",
        "    experiment_results.append(result)\n",
        "\n",
        "all_metrics_df = calculate_all_convergence_metrics(experiment_results)\n",
        "all_metrics_df.to_csv(convergence_dir / 'multi_rho_convergence_metrics.csv', index=False)\n",
        "\n",
        "fig = plot_convergence_multi_rho(experiment_results, metrics=['rl2', 'loss'])\n",
        "fig.savefig(convergence_dir / 'multi_rho_convergence_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "write_experiment_summary(\n",
        "    experiment_results,\n",
        "    output_file=convergence_dir / 'fine_tuning_summary.txt',\n",
        "    threshold=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGQNUu5J9qh4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gnn_project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
