{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install wandb in colab (if required)\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login (if required)\n",
    "import wandb\n",
    "\n",
    "### WANDB PROJECT NAME !!!\n",
    "wandb_project_name = \"gnn_1d_logistic_repr_regime\"\n",
    "wandb.login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import random\n",
    "from torch.optim import LBFGS, Adam\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# make sure that util is correctly accessed from parent directory\n",
    "ppp_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if ppp_dir not in sys.path:\n",
    "    sys.path.insert(0, ppp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class WaveAct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaveAct, self).__init__() \n",
    "        self.w1 = nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "        self.w2 = nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w1 * torch.sin(x)+ self.w2 * torch.cos(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=256):\n",
    "        super(FeedForward, self).__init__() \n",
    "        self.linear = nn.Sequential(*[\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.act1 = WaveAct()\n",
    "        self.act2 = WaveAct()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.act1(x)\n",
    "        # pdb.set_trace()\n",
    "        x = x + self.attn(x2,x2,x2)[0]\n",
    "        x2 = self.act2(x)\n",
    "        x = x + self.ff(x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.act1 = WaveAct()\n",
    "        self.act2 = WaveAct()\n",
    "\n",
    "    def forward(self, x, e_outputs): \n",
    "        x2 = self.act1(x)\n",
    "        x = x + self.attn(x2, e_outputs, e_outputs)[0]\n",
    "        x2 = self.act2(x)\n",
    "        x = x + self.ff(x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, N, heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.act = WaveAct()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, N, heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.act = WaveAct()\n",
    "        \n",
    "    def forward(self, x, e_outputs):\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "class PINNsformer(nn.Module):\n",
    "    def __init__(self, d_out, d_model, d_hidden, N, heads):\n",
    "        \"\"\"\n",
    "        Adapted PINNsformer that takes three inputs: x, t, and rho.\n",
    "        Args:\n",
    "            d_out (int): Output dimension.\n",
    "            d_model (int): Dimension of the model embeddings.\n",
    "            d_hidden (int): Hidden layer dimension in the output MLP.\n",
    "            N (int): Number of encoder/decoder layers.\n",
    "            heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(PINNsformer, self).__init__()\n",
    "        # Change input dimension from 2 to 3 to accommodate x, t, and rho\n",
    "        self.linear_emb = nn.Linear(3, d_model)\n",
    "\n",
    "        self.encoder = Encoder(d_model, N, heads)\n",
    "        self.decoder = Decoder(d_model, N, heads)\n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Linear(d_model, d_hidden),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_hidden, d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, rho):\n",
    "        # Concatenate x, t, and rho along the last dimension\n",
    "        src = torch.cat((x, t, rho), dim=-1)\n",
    "        src = self.linear_emb(src)\n",
    "\n",
    "        e_outputs = self.encoder(src)\n",
    "        d_output = self.decoder(src, e_outputs)\n",
    "        output = self.linear_out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEData(Dataset):\n",
    "    def __init__(self, t_range, rho_values, t_points, constant_x, device='cuda:0', use_time_sequencing=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset for a logistic growth ODE with a constant spatial coordinate.\n",
    "\n",
    "        Args:\n",
    "            t_range (list): Time domain [t_min, t_max].\n",
    "            rho_values (list): List of reaction coefficients (? values).\n",
    "            t_points (int): Number of time points.\n",
    "            constant_x (float): The constant spatial coordinate (e.g., a representative location).\n",
    "            device (str): Device to store the tensors ('cpu' or 'cuda:0').\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.t_range = t_range\n",
    "        self.rho_values = rho_values\n",
    "        self.t_points = t_points\n",
    "        self.constant_x = constant_x\n",
    "        self.use_time_sequencing = use_time_sequencing\n",
    "\n",
    "        # Prepare data for each rho value.\n",
    "        self.data = {}\n",
    "        for rho in rho_values:\n",
    "            # Generate residual points (time samples with constant x)\n",
    "\n",
    "            res, ic = self._generate_data()\n",
    "            if self.use_time_sequencing == True: \n",
    "                res = self._make_time_sequence(res)\n",
    "                ic = self._make_time_sequence(ic)\n",
    "\n",
    "            res_tensor = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            ic_tensor = torch.tensor(ic, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            \n",
    "            # Precompute analytical solution at the initial condition (t = t_range[0])\n",
    "            u_ic = self.analytical_solution(\n",
    "                torch.tensor([[constant_x]], dtype=torch.float32, requires_grad=True).to(self.device),\n",
    "                torch.tensor([[t_range[0]]], dtype=torch.float32, requires_grad=True).to(self.device),\n",
    "                rho\n",
    "            )\n",
    "            \n",
    "            self.data[rho] = {\n",
    "                'res': res_tensor,   # (x, t) pairs over the time domain (x is constant)\n",
    "                'ic': ic_tensor,     # Initial condition point (t = t_range[0])\n",
    "                'u_ic': u_ic         # Analytical solution at t = t_range[0]\n",
    "            }\n",
    "\n",
    "    def _generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate residual points (for the interior of the time domain) and the initial condition.\n",
    "        \n",
    "        Returns:\n",
    "            res (np.ndarray): Array of shape (t_points, 2) where each row is [constant_x, t].\n",
    "            ic (np.ndarray): Array of shape (1, 2) corresponding to the initial condition at t = t_range[0].\n",
    "        \"\"\"\n",
    "        # Create time samples\n",
    "        t = np.linspace(self.t_range[0], self.t_range[1], self.t_points)\n",
    "        # For each t, x is always the constant value provided.\n",
    "        x = self.constant_x * np.ones_like(t)\n",
    "        # Stack x and t to create our (x,t) pairs.\n",
    "        res = np.stack([x, t], axis=-1)  # Shape: (t_points, 2)\n",
    "        # The initial condition is simply the first row.\n",
    "        ic = res[0:1, :]\n",
    "        return res, ic\n",
    "\n",
    "    def analytical_solution(self, x, t, rho):\n",
    "        \"\"\"\n",
    "        Compute the analytical solution for the logistic growth ODE.\n",
    "        Here we use the same functional form as before:\n",
    "        \n",
    "        u(t) = h(x) * exp(? t) / (h(x) * exp(? t) + 1 - h(x)),  with\n",
    "        h(x) = exp( - (x - ?)? / [2*(?/4)?] ).\n",
    "        \n",
    "        Note: Since x is constant, h(x) is also constant.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The spatial input (constant value).\n",
    "            t (torch.Tensor): Time input.\n",
    "            rho (float): Reaction coefficient.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The analytical solution.\n",
    "        \"\"\"\n",
    "        pi = torch.tensor(np.pi, dtype=torch.float32, device=self.device)\n",
    "        h = torch.exp(- (x - pi)**2 / (2 * (pi / 4)**2))\n",
    "        return h * torch.exp(rho * t) / (h * torch.exp(rho * t) + 1 - h)\n",
    "\n",
    "    def get_test_points(self, rho):\n",
    "        \"\"\"\n",
    "        For this simple ODE experiment, the test points are the same as the interior points.\n",
    "        \n",
    "        Returns:\n",
    "            x, t, rho tensor.\n",
    "        \"\"\"\n",
    "        return self.get_interior_points(rho) \n",
    "    \n",
    "    def _make_time_sequence(self, src, num_step=5, step=1e-4):\n",
    "        dim = num_step\n",
    "        src = np.repeat(np.expand_dims(src, axis=1), dim, axis=1)  # (N, L, 2)\n",
    "        for i in range(num_step):\n",
    "            src[:,i,-1] += step*i\n",
    "        return src\n",
    "    \n",
    "    def get_interior_points(self, rho):\n",
    "        \"\"\"\n",
    "        Retrieve the interior (residual) points for a given rho.\n",
    "        \"\"\"\n",
    "        res = self.data[rho]['res']\n",
    "        if self.use_time_sequencing:\n",
    "            x = res[:, :, 0:1]  # Shape: (N, num_step, 1)\n",
    "            t = res[:, :, 1:2]  # Shape: (N, num_step, 1)\n",
    "        else:\n",
    "            x = res[:, 0:1]\n",
    "            t = res[:, 1:2]\n",
    "        \n",
    "        # Create appropriate rho tensor\n",
    "        if self.use_time_sequencing:\n",
    "            rho_tensor = torch.full_like(x, rho)\n",
    "        else:\n",
    "            rho_tensor = torch.full_like(x, rho)\n",
    "        \n",
    "        return x, t, rho_tensor\n",
    "\n",
    "    def get_initial_condition(self, rho):\n",
    "        \"\"\"\n",
    "        Retrieve the initial condition point and its analytical solution.\n",
    "        \"\"\"\n",
    "        ic = self.data[rho]['ic']\n",
    "        if self.use_time_sequencing:\n",
    "            x_ic = ic[:, :, 0:1]  # Shape: (1, num_step, 1)\n",
    "            t_ic = ic[:, :, 1:2]  # Shape: (1, num_step, 1)\n",
    "        else:\n",
    "            x_ic = ic[:, 0:1]\n",
    "            t_ic = ic[:, 1:2]\n",
    "        \n",
    "        # Create appropriate rho tensor\n",
    "        if self.use_time_sequencing:\n",
    "            rho_tensor = torch.full_like(x_ic, rho)\n",
    "        else:\n",
    "            rho_tensor = torch.full_like(x_ic, rho)\n",
    "        \n",
    "        u_ic = self.data[rho]['u_ic']\n",
    "        \n",
    "        return x_ic, t_ic, rho_tensor, u_ic\n",
    "\n",
    "    def get_interior_input_without_points(self):\n",
    "        \"\"\"\n",
    "        Generate interior input points dynamically without precomputed values.\n",
    "        \"\"\"\n",
    "        res, _ = self._generate_data()\n",
    "\n",
    "        if self.use_time_sequencing:\n",
    "            res = self._make_time_sequence(res)\n",
    "            res_tensor = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            x = res_tensor[:, :, 0:1]  # Shape: (N, num_step, 1) \n",
    "            t = res_tensor[:, :, 1:2]  # Shape: (N, num_step, 1)\n",
    "        else:\n",
    "            res_tensor = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            x = res_tensor[:, 0:1]\n",
    "            t = res_tensor[:, 1:2]\n",
    "\n",
    "        return x, t, None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperparamConfig:\n",
    "    # General parameters\n",
    "    total_i: int = None\n",
    "    dataset: str = None\n",
    "    model_name: str = None\n",
    "    in_dim: int = None\n",
    "    hidden_dim: int = None\n",
    "    out_dim: int = None\n",
    "    num_layer: int = None\n",
    "    init_weights: str = None\n",
    "    bias_fill: float = None\n",
    "\n",
    "    # Model-specific parameters (only relevant if model_name is \"pinn_ff\")\n",
    "    init_activ_func: Optional[str] = None  # Only relevant for \"pinn_ff\"       # sin , tanh, gelu\n",
    "    subseq_activ_func: Optional[str] = None  # Only relevant for \"pinn_ff\"     # tanh, gelu\n",
    "\n",
    "    # Only relevant if model_name is \"pinnsformer\"\n",
    "    emb_dim: Optional[int] = field(default=None) \n",
    "    num_heads: Optional[int] = field(default=None)\n",
    "\n",
    "    # Optimizer parameters\n",
    "    optimizer: str = \"adam\"  # Options: \"adam\", \"lbfgs\"\n",
    "    learning_rate: Optional[float] = field(default=None)  # Only relevant for Adam\n",
    "    batch_size: Optional[int] = field(default=None)  # Only relevant for Adam\n",
    "\n",
    "    # LBFGS-specific parameters\n",
    "    max_iter: Optional[int] = field(default=None)  # Only relevant for LBFGS\n",
    "    line_search_fn: Optional[str] = field(default=None)  # Only relevant for LBFGS\n",
    "\n",
    "    # Normalization parameters\n",
    "    normalize_res: bool = False\n",
    "    normalize_ic: bool = False\n",
    "    alpha: Optional[float] = None  # Only relevant if normalize_res or normalize_ic is True\n",
    "    epsilon: Optional[float] = None  # Only relevant if normalize_res or normalize_ic is True\n",
    "\n",
    "    # Adaptive loss weighting \n",
    "    adaptive_loss_weighting: bool = False \n",
    "    adaptive_loss_coeff: Optional[float] = field(default=None)  \n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validate the configuration to ensure all conditional parameters are set correctly.\n",
    "        \"\"\"\n",
    "        # Validate optimizer-specific parameters\n",
    "        if self.optimizer == \"adam\":\n",
    "            if self.learning_rate is None:\n",
    "                raise ValueError(\"`learning_rate` must be specified when optimizer is 'adam'.\")\n",
    "            if self.batch_size is None:\n",
    "                raise ValueError(\"`batch_size` must be specified when optimizer is 'adam'.\")\n",
    "        elif self.optimizer == \"lbfgs\":\n",
    "            if self.max_iter is None:\n",
    "                raise ValueError(\"`max_iter` must be specified when optimizer is 'lbfgs'.\")\n",
    "            if self.line_search_fn is None:\n",
    "                raise ValueError(\"`line_search_fn` must be specified when optimizer is 'lbfgs'.\")\n",
    "\n",
    "        # Validate model-specific parameters\n",
    "        if self.model_name == \"pinn_ff\":\n",
    "            if self.init_activ_func is None:\n",
    "                raise ValueError(\"`init_activ_func` must be specified when model_name is 'pinn_ff'.\")\n",
    "            if self.subseq_activ_func is None:\n",
    "                raise ValueError(\"`subseq_activ_func` must be specified when model_name is 'pinn_ff'.\")\n",
    "            \n",
    "        if self.model_name == \"pinnsformer\":\n",
    "            if self.emb_dim is None:\n",
    "                raise ValueError(\"`emb_dim` must be specified when model_name is 'pinnsformer'.\")\n",
    "            if self.num_heads is None:\n",
    "                raise ValueError(\"`num_heads` must be specified when model_name is 'pinnsformer'.\")\n",
    "\n",
    "\n",
    "        # Validate normalization-specific parameters\n",
    "        if self.normalize_res or self.normalize_ic:\n",
    "            if self.alpha is None:\n",
    "                raise ValueError(\"`alpha` must be specified when normalize_res or normalize_ic is True.\")\n",
    "            if self.epsilon is None:\n",
    "                raise ValueError(\"`epsilon` must be specified when normalize_res or normalize_ic is True.\")\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Convert the dataclass to a dictionary for use with wandb or other logging tools.\n",
    "        \"\"\"\n",
    "        return {k: v for k, v in self.__dict__.items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\".\") / \"repr_regime\" # Base directory for results\n",
    "images_dir = base_dir / \"images\"  # Subdirectory for images\n",
    "weights_dir = base_dir / \"weights\"  # Subdirectory for stored model\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "weights_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If CUDA is available, print the CUDA version\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    device = 'cuda:0'\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure initial dataset and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = HyperparamConfig(\n",
    "    total_i=10,\n",
    "    dataset=\"1d-logistic-ode\",\n",
    "    model_name=\"pinnsformer\",  # Model name\n",
    "    in_dim=3,\n",
    "    hidden_dim=256,\n",
    "    out_dim=1,\n",
    "    emb_dim=32,\n",
    "    num_layer=1,\n",
    "    num_heads=2,\n",
    "    init_weights=\"xavier uniform\",\n",
    "    bias_fill=0.01,\n",
    "    optimizer=\"adam\",\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Validate the configuration\n",
    "config.validate()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(config.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hyperparameter Tuning: Small $\\rho$ parameter variance datasets, adam optimizer with mini batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rho_values(start, end, num=10):\n",
    "    return np.linspace(start, end, num).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_errors(model, dataset, rho, device):\n",
    "    # Get test points\n",
    "    x, t, rho_tensor = dataset.get_test_points(rho)\n",
    "    \n",
    "    # Compute predictions\n",
    "    with torch.no_grad():\n",
    "        pred = model(x, t, rho_tensor).detach().cpu().numpy()\n",
    "    \n",
    "    # Compute analytical solution\n",
    "    with torch.no_grad():\n",
    "        u_analytical = dataset.analytical_solution(x, t, rho).detach().cpu().numpy()\n",
    "    \n",
    "    # Handle time sequencing case (shapes will be different)\n",
    "    if dataset._use_time_sequencing:\n",
    "        # Option 1: Reshape predictions to match analytical solution\n",
    "        if pred.shape != u_analytical.shape:\n",
    "            if len(pred.shape) == 3:  # If pred has shape (N, num_step, 1)\n",
    "                pred = pred.reshape(-1, 1)  # Flatten to (N*num_step, 1)\n",
    "            if len(u_analytical.shape) == 3:  # If u_analytical has shape (N, num_step, 1)\n",
    "                u_analytical = u_analytical.reshape(-1, 1)  # Flatten to (N*num_step, 1)\n",
    "    \n",
    "    # Compute relative errors\n",
    "    rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
    "    rl2 = np.sqrt(np.sum((u_analytical - pred) ** 2) / np.sum(u_analytical ** 2))\n",
    "    \n",
    "    return rl1, rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper Functions ===\n",
    "def compute_grad_norm(loss, model):\n",
    "    \"\"\"Compute and normalize gradient of loss w.r.t. model parameters.\"\"\"\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    flat_grad = torch.cat([g.view(-1) for g in grads if g is not None])\n",
    "    return flat_grad / (flat_grad.norm() + 1e-8)\n",
    "\n",
    "def update_momentum(grads, momentum_dict, alpha):\n",
    "    \"\"\"Exponential moving average update for gradient momentum.\"\"\"\n",
    "    for key in grads:\n",
    "        if key not in momentum_dict:\n",
    "            momentum_dict[key] = grads[key].detach().clone()\n",
    "        else:\n",
    "            momentum_dict[key] = alpha * momentum_dict[key] + (1 - alpha) * grads[key].detach()\n",
    "    return momentum_dict\n",
    "\n",
    "def compute_loss_weights(momentum_dict):\n",
    "    \"\"\"Compute inverse-norm weights from gradient momenta.\"\"\"\n",
    "    weights = {k: 1.0 / (v.norm() + 1e-8) for k, v in momentum_dict.items()}\n",
    "    total = sum(weights.values())\n",
    "    return {k: weights[k] / total for k in weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper Function for One Training Run ===\n",
    "def run_training(config_dict, model, optim, dataset, rho_values, device):\n",
    "    \"\"\"\n",
    "    Perform one training run based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        config_dict (dict): Configuration dictionary containing hyperparameters.\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        dataset: Dataset object providing interior and initial condition points.\n",
    "        rho_values (list): List of rho values for training.\n",
    "        device (torch.device): Device to run the training on (e.g., 'cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    wandb.init(\n",
    "        project=wandb_project_name,\n",
    "        config=config_dict,\n",
    "        settings=wandb.Settings(silent=True)\n",
    "    )\n",
    "\n",
    "    momentum_dict = {}  # Reset for each run\n",
    "    loss_track = {}\n",
    "    model.train() # set to training mode\n",
    "\n",
    "    for i in tqdm(range(config_dict[\"total_i\"])):\n",
    "        total_loss_res = 0.0\n",
    "        total_loss_ic = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for rho in rho_values:\n",
    "            x_res, t_res, rho_res = dataset.get_interior_points(rho)\n",
    "            interior_dataset = TensorDataset(x_res, t_res, rho_res)\n",
    "            interior_loader = DataLoader(interior_dataset, batch_size=config_dict[\"batch_size\"], shuffle=True)\n",
    "\n",
    "            x_ic, t_ic, rho_ic, u_ic = dataset.get_initial_condition(rho)\n",
    "\n",
    "            for bx, bt, brho in interior_loader:\n",
    "                bx.requires_grad_()\n",
    "                bt.requires_grad_()\n",
    "\n",
    "                pred_res = model(bx.to(device), bt.to(device), brho.to(device))\n",
    "                u_t = torch.autograd.grad(pred_res, bt.to(device), grad_outputs=torch.ones_like(pred_res),\n",
    "                                          retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "                if config_dict[\"normalize_res\"]:\n",
    "                    normalized_res_error = (u_t - brho * pred_res * (1 - pred_res)) / (\n",
    "                        config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
    "                    loss_res = torch.mean(normalized_res_error ** 2)\n",
    "                else:\n",
    "                    loss_res = torch.mean((u_t - brho * pred_res * (1 - pred_res)) ** 2)\n",
    "\n",
    "                pred_ic = model(x_ic.to(device), t_ic.to(device), rho_ic.to(device))\n",
    "\n",
    "                if config_dict[\"normalize_ic\"]:\n",
    "                    normalized_ic_error = (pred_ic - u_ic.to(device)) / (\n",
    "                        config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
    "                    loss_ic = torch.mean(normalized_ic_error ** 2)\n",
    "                else:\n",
    "                    loss_ic = torch.mean((pred_ic - u_ic.to(device)) ** 2)\n",
    "\n",
    "                if config_dict[\"adaptive_loss_weighting\"]:\n",
    "                    grad_res = compute_grad_norm(loss_res, model)\n",
    "                    grad_ic = compute_grad_norm(loss_ic, model)\n",
    "\n",
    "                    grads = {'res': grad_res, 'ic': grad_ic}\n",
    "                    momentum_dict = update_momentum(grads, momentum_dict, config_dict[\"adaptive_loss_coeff\"])\n",
    "                    gamma = compute_loss_weights(momentum_dict)\n",
    "\n",
    "                    loss = gamma['res'] * loss_res + gamma['ic'] * loss_ic\n",
    "                else:\n",
    "                    loss = loss_res + loss_ic\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optim.step()\n",
    "\n",
    "                total_loss_res += loss_res.item()\n",
    "                total_loss_ic += loss_ic.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        avg_loss_res = total_loss_res / num_batches\n",
    "        avg_loss_ic = total_loss_ic / num_batches\n",
    "        avg_total_loss = avg_loss_res + avg_loss_ic\n",
    "\n",
    "        wandb_dict = {\n",
    "            \"iteration\": i,\n",
    "            \"avg_loss_res\": avg_loss_res,\n",
    "            \"avg_loss_ic\": avg_loss_ic,\n",
    "            \"avg_total_loss\": avg_total_loss\n",
    "        }\n",
    "\n",
    "        total_l1 = 0.0\n",
    "        total_l2 = 0.0\n",
    "        for rho in rho_values:\n",
    "            rl1, rl2 = compute_relative_errors(model, dataset, rho, device)\n",
    "            wandb_dict[f\"{rho}_rl1\"] = rl1\n",
    "            wandb_dict[f\"{rho}_rl2\"] = rl2\n",
    "            total_l1 += rl1 \n",
    "            total_l2 += rl2\n",
    "\n",
    "        average_l1 = total_l1 / len(rho_values) \n",
    "        average_l2 = total_l2 / len(rho_values)\n",
    "\n",
    "        wandb.log(wandb_dict)\n",
    "        loss_track[i] = wandb_dict\n",
    "\n",
    "    wandb.finish()\n",
    "    return loss_track, average_l1, average_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(config.bias_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_and_errors(rho_values, predictions, analytical_solutions, errors, num_cols=4, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot predictions and errors for multiple rho values in a grid layout.\n",
    "\n",
    "    Args:\n",
    "        rho_values (list): List of rho values.\n",
    "        predictions (dict): Dictionary of predictions for each rho value.\n",
    "        analytical_solutions (dict): Dictionary of analytical solutions for each rho value.\n",
    "        errors (dict): Dictionary of absolute errors for each rho value.\n",
    "        num_cols (int): Number of columns in the grid (default: 4).\n",
    "        save_path (str or Path, optional): Path to save the figure. If None, the figure is not saved.\n",
    "    \"\"\"\n",
    "    num_rho = len(rho_values)\n",
    "    num_rows = 2  # Fixed: Row 1 for predictions, Row 2 for errors\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 6))\n",
    "\n",
    "    # Ensure axes is a 2D array even if num_cols == 1\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for idx, rho in enumerate(rho_values):\n",
    "        col = idx % num_cols\n",
    "\n",
    "        # Extract data for the current rho\n",
    "        pred = predictions[rho]\n",
    "        analytical = analytical_solutions[rho]\n",
    "        abs_error = errors[rho]\n",
    "        percentage_error = (abs_error / np.maximum(analytical, 1e-8)) * 100  # Avoid division by zero\n",
    "\n",
    "        ax_pred = axes[0, col]\n",
    "        ax_pred.plot(pred, label=\"Prediction\", color=\"blue\", linewidth=2)\n",
    "        ax_pred.plot(analytical, label=\"Analytical\", color=\"orange\", linestyle=\"dashed\", linewidth=2)\n",
    "        ax_pred.set_title(f\"Rho: {rho} - Prediction\")\n",
    "        ax_pred.set_xlabel(\"t - Time\")  # Horizontal axis label\n",
    "        ax_pred.set_ylabel(\"u(t) - Value\")  # Vertical axis label\n",
    "        ax_pred.legend()\n",
    "        \n",
    "\n",
    "        # Plot absolute and percentage errors (Row 2)\n",
    "        ax_err = axes[1, col]\n",
    "        #ax_err.plot(abs_error, label=\"Absolute Error\", color=\"red\", linewidth=2)\n",
    "        ax_err.plot(percentage_error, label=\"Percentage Error\", color=\"green\", linestyle=\"dotted\", linewidth=2)\n",
    "        ax_err.set_title(f\"Rho: {rho} - Relative Error (%)\")\n",
    "        ax_err.set_xlabel(\"t - Time\")  # Horizontal axis label\n",
    "        ax_err.set_ylabel(\"delta u(t) (%)\")  # Vertical axis label\n",
    "        ax_err.legend()\n",
    "\n",
    "    # Hide unused subplots if num_rho < num_cols\n",
    "    for idx in range(num_rho, num_cols):\n",
    "        axes[0, idx].axis(\"off\")\n",
    "        axes[1, idx].axis(\"off\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure if a save path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_parameter_range(rho_values, dataset, model):\n",
    "    predictions = {}\n",
    "    analytical_solutions = {}\n",
    "    errors = {}\n",
    "\n",
    "    for rho in rho_values:\n",
    "        # Get test points for the current rho\n",
    "        x_test, t_test, _ = dataset.get_test_points(rho)\n",
    "        rho_test = torch.full_like(x_test, rho).to(device)\n",
    "\n",
    "        # Compute analytical solution\n",
    "        u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        # Compute best model predictions\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "            pred = pred.cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        # Compute error\n",
    "        error = np.abs(u_analytical - pred)\n",
    "\n",
    "        # Store results\n",
    "        predictions[rho] = pred\n",
    "        analytical_solutions[rho] = u_analytical\n",
    "        errors[rho] = error\n",
    "\n",
    "    return predictions, analytical_solutions, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(config, dataset, rho_values, device, models, normalize_res_values, normalize_ic_values, adaptive_loss, alpha_values, epsilon_values, iteration_steps):\n",
    "    \"\"\"\n",
    "    Perform a grid search over model and hyperparameter variations to find the best model.\n",
    "\n",
    "    Args:\n",
    "        config (HyperparamConfig): Configuration object containing hyperparameters.\n",
    "        dataset (ODEData): Dataset object for training and evaluation.\n",
    "        rho_values (list): List of rho values for training and evaluation.\n",
    "        device (torch.device): Device to run the training on (e.g., 'cuda:0').\n",
    "        models (list): List of model variations (tuples of model and hyperparameters).\n",
    "        normalize_res_values (list): List of boolean values for normalizing residuals.\n",
    "        normalize_ic_values (list): List of boolean values for normalizing initial conditions.\n",
    "        adaptive_loss (list): List of boolean values for enabling adaptive loss weighting.\n",
    "        adaptive_loss_weighting_coeffs (list): List of coefficients for adaptive loss weighting.\n",
    "        alpha_values (list): List of alpha values for normalization.\n",
    "        epsilon_values (list): List of epsilon values for normalization.\n",
    "        iteration_steps (list): List of iteration steps for training.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_model, best_config, best_l1, best_l2)\n",
    "            - best_model: The model with the best performance.\n",
    "            - best_config: The configuration of the best model.\n",
    "            - best_l1: The lowest average L1 error achieved.\n",
    "            - best_l2: The lowest average L2 error achieved.\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_l1 = float('inf')\n",
    "    best_l2 = float('inf')\n",
    "\n",
    "    for model, emb_dim, num_heads, hidden_dim,num_layer in models:\n",
    "        for normalize_res in normalize_res_values:\n",
    "            for normalize_ic in normalize_ic_values:\n",
    "                for ad_loss,a_coeff in adaptive_loss:\n",
    "                    for alpha in alpha_values:\n",
    "                        for epsilon in epsilon_values:\n",
    "                            for total_i in iteration_steps:\n",
    "                                # Update the configuration\n",
    "                                config.normalize_res = normalize_res\n",
    "                                config.normalize_ic = normalize_ic\n",
    "                                config.adaptive_loss_coeff = a_coeff\n",
    "                                config.alpha = alpha\n",
    "                                config.epsilon = epsilon\n",
    "                                config.total_i = total_i\n",
    "                                config.adaptive_loss_weighting = ad_loss\n",
    "                                config.num_layer = num_layer\n",
    "                                config.hidden_dim = hidden_dim\n",
    "                                config.emb_dim = emb_dim\n",
    "                                config.num_heads = num_heads\n",
    "                                config.validate()\n",
    "\n",
    "                                # Call the training function\n",
    "                                config_dict = config.to_dict()\n",
    "                                if config.optimizer == \"adam\":\n",
    "                                    optim = Adam(model.parameters(), lr=config_dict[\"learning_rate\"])\n",
    "                                loss_track, avg_l1, avg_l2 = run_training(config_dict, model, optim, dataset, rho_values, device)\n",
    "\n",
    "                                # Update the best model if the current one is better\n",
    "                                if avg_l1 < best_l1 and avg_l2 < best_l2:\n",
    "                                    best_model = model\n",
    "                                    best_config = copy.deepcopy(config)\n",
    "\n",
    "                                    best_l1 = avg_l1\n",
    "                                    best_l2 = avg_l2\n",
    "\n",
    "    return best_model, best_config, best_l1, best_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning through grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning step 1\n",
    "\n",
    "We explore the impact of the following params on convergence and performance:\n",
    "1. adaptive_loss - on/off\n",
    "2. different hidden sizes\n",
    "3. different head counts\n",
    "\n",
    "Other params remain unchanged:\n",
    "- `iteration_steps` - **fixed value**!\n",
    "- `embedding_dim` - **fixed value**!\n",
    "- `num_layers` - **fixed value**!\n",
    "\n",
    "**Also:** Here we use a parameter regime (0.5-4.0) that is **representative** of all 3 regimes to train the model (Hence `repr` script). See `1d_logistic_ode_regimes.ipynb` for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rho values\n",
    "start = 0.5 \n",
    "end = 4.0\n",
    "rho_values = generate_rho_values(start, end, num=5)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ODEData(t_range=[0, 1], rho_values=rho_values, t_points=101, constant_x=1.0, device='cuda:0', use_time_sequencing=True)\n",
    "\n",
    "# Define variations\n",
    "normalize_res_values = [False]  # [True, False]\n",
    "normalize_ic_values = [False]  # [True, False]\n",
    "adaptive_loss = [(True,0.9), (False,None)]\n",
    "#adaptive_loss_weighting_coeffs = [0.9]#[0.7, 0.9]\n",
    "alpha_values = [None]  # [0.1, 0.25]\n",
    "epsilon_values = [None]  # [0.1, 0.3]\n",
    "iteration_steps = [50] #[300, 500]  # [50, 100, 400]\n",
    "\n",
    "# Define model variations\n",
    "models = []\n",
    "model_variations = [\n",
    "    (32, 1, 64, 1),\n",
    "    (32, 2, 64, 1),\n",
    "    (32, 4, 64, 1),\n",
    "    (32, 1, 128, 1),\n",
    "    (32, 2, 128, 1),\n",
    "    (32, 4, 128, 1),\n",
    "    (32, 1, 256, 1),\n",
    "    (32, 2, 256, 1),\n",
    "    (32, 4, 256, 1),\n",
    "    (32, 1, 512, 1),\n",
    "]\n",
    "for emb_dim, num_heads, hidden_dim, num_layer in model_variations:\n",
    "    model = PINNsformer(\n",
    "        d_out=config.out_dim,\n",
    "        d_model=emb_dim,\n",
    "        d_hidden=hidden_dim,\n",
    "        N=num_layer,\n",
    "        heads=num_heads\n",
    "    ).to(device)\n",
    "    model.apply(init_weights)\n",
    "    models.append((model, emb_dim, num_heads, hidden_dim, num_layer))\n",
    "\n",
    "# Perform grid search\n",
    "best_model, best_config, best_l1, best_l2 = perform_grid_search(\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    rho_values=rho_values,\n",
    "    device='cuda:0',\n",
    "    models=models,\n",
    "    normalize_res_values=normalize_res_values,\n",
    "    normalize_ic_values=normalize_ic_values,\n",
    "    adaptive_loss=adaptive_loss,\n",
    "    alpha_values=alpha_values,\n",
    "    epsilon_values=epsilon_values,\n",
    "    iteration_steps=iteration_steps\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Model Configuration:\")\n",
    "print(best_config)\n",
    "print(f\"Best Average L1 Error: {best_l1}\")\n",
    "print(f\"Best Average L2 Error: {best_l2}\")\n",
    "# Save the best model\n",
    "model_path = weights_dir / \"1d_logistic_ode_repr_regime_pinnsformer_v1.pt\"  # Path to save the model\n",
    "torch.save(best_model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "predictions, analytical_solutions, errors = test_on_parameter_range(rho_values, dataset, best_model)\n",
    "\n",
    "# Plot predictions and errors\n",
    "plot_predictions_and_errors(\n",
    "    rho_values=rho_values,\n",
    "    predictions=predictions,\n",
    "    analytical_solutions=analytical_solutions,\n",
    "    errors=errors,\n",
    "    num_cols=len(rho_values),\n",
    "    save_path=images_dir / \"predictions_and_errors_pinnsformer_v1.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning step 2\n",
    "The previous results show, that higher `hidden_dim` values\n",
    "are in fact not helpful. But increasing `head_num` is useful.\n",
    "\n",
    "\n",
    "We explore the impact of the following params on convergence and performance:\n",
    "1. adaptive_loss - on/off\n",
    "2. only small hidden sizes (64,128)\n",
    "3. different head counts\n",
    "4. increasing number of iteration steps\n",
    "\n",
    "Other params remain unchanged:\n",
    "- `embedding_dim` - **fixed value**!\n",
    "- `num_layers` - **fixed value**!\n",
    "\n",
    "**Also:** Here we use a parameter regime (0.5-4.0) that is **representative** of all 3 regimes to train the model (Hence `repr` script). See `1d_logistic_ode_regimes.ipynb` for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rho values\n",
    "start = 0.5 \n",
    "end = 4.0\n",
    "rho_values = generate_rho_values(start, end, num=5)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ODEData(t_range=[0, 1], rho_values=rho_values, t_points=101, constant_x=1.0, device='cuda:0', use_time_sequencing=True)\n",
    "\n",
    "# Define variations\n",
    "normalize_res_values = [False]  # [True, False]\n",
    "normalize_ic_values = [False]  # [True, False]\n",
    "adaptive_loss = [(True,0.9), (False,None)]\n",
    "#adaptive_loss_weighting_coeffs = [0.9]#[0.7, 0.9]\n",
    "alpha_values = [None]  # [0.1, 0.25]\n",
    "epsilon_values = [None]  # [0.1, 0.3]\n",
    "iteration_steps = [200, 300, 500]  # [50, 100, 400]\n",
    "\n",
    "# Define model variations\n",
    "models = []\n",
    "model_variations = [\n",
    "    (32, 1, 64, 1),\n",
    "    (32, 2, 64, 1),\n",
    "    (32, 2, 128, 1),\n",
    "    (32, 1, 128, 1),\n",
    "]\n",
    "for emb_dim, num_heads, hidden_dim, num_layer in model_variations:\n",
    "    model = PINNsformer(\n",
    "        d_out=config.out_dim,\n",
    "        d_model=emb_dim,\n",
    "        d_hidden=hidden_dim,\n",
    "        N=num_layer,\n",
    "        heads=num_heads\n",
    "    ).to(device)\n",
    "    model.apply(init_weights)\n",
    "    models.append((model, emb_dim, num_heads, hidden_dim, num_layer))\n",
    "\n",
    "# Perform grid search\n",
    "best_model, best_config, best_l1, best_l2 = perform_grid_search(\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    rho_values=rho_values,\n",
    "    device='cuda:0',\n",
    "    models=models,\n",
    "    normalize_res_values=normalize_res_values,\n",
    "    normalize_ic_values=normalize_ic_values,\n",
    "    adaptive_loss=adaptive_loss,\n",
    "    alpha_values=alpha_values,\n",
    "    epsilon_values=epsilon_values,\n",
    "    iteration_steps=iteration_steps\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Model Configuration:\")\n",
    "print(best_config)\n",
    "print(f\"Best Average L1 Error: {best_l1}\")\n",
    "print(f\"Best Average L2 Error: {best_l2}\")\n",
    "# Save the best model\n",
    "model_path = weights_dir / \"1d_logistic_ode_low_regime_pinnsformer_v2.pt\"  # Path to save the model\n",
    "torch.save(best_model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing best model on different rho ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add additional import for visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model configuration:**\n",
    "\n",
    "{'total_i': 400, 'dataset': '1d-logistic-ode', 'model_name': 'pinn_ff', 'in_dim': 3, 'hidden_dim': 256, 'out_dim': 1, 'num_layer': 4, 'init_weights': 'xavier uniform', 'bias_fill': 0.01, 'init_activ_func': 'tanh', 'subseq_activ_func': 'gelu', 'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 128, 'normalize_res': False, 'normalize_ic': False, 'adaptive_loss_weighting': True, 'adaptive_loss_coeff': 0.9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_model = False\n",
    "\n",
    "if load_existing_model == True:\n",
    "    best_model = PINNsformer(\n",
    "                    d_out=best_config.out_dim,\n",
    "                    d_model=best_config.emb_dim,\n",
    "                    d_hidden=best_config.hidden_dim,\n",
    "                    N=best_config.num_layer,\n",
    "                    heads=best_config.num_heads).to(device)\n",
    "\n",
    "    model_path = weights_dir / \"1d_logistic_ode_fls_extended.pt\"  # Path to the saved model\n",
    "    best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    best_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Generate uniformly spaced ρ values in a given range\n",
    "def generate_rho_values(start, end, num=10):\n",
    "    return np.linspace(start, end, num).tolist()\n",
    "\n",
    "# Function 2: Evaluate model on ρ values and return error stats\n",
    "def evaluate_rho_range(rho_values, dataset, model, device='cpu'):\n",
    "    results = []\n",
    "\n",
    "    for rho in rho_values:\n",
    "        x_test, t_test, _ = dataset.get_interior_input_without_points()\n",
    "        rho_test = torch.full_like(x_test, rho).to(device)\n",
    "\n",
    "        u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0]\n",
    "            pred = pred.cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
    "        rl2 = np.sqrt(np.sum((u_analytical - pred) ** 2) / np.sum(u_analytical ** 2))\n",
    "\n",
    "        results.append({'rho': rho, 'rl1': rl1, 'rl2': rl2})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Function 3: Plot scatter and compute Pearson correlation\n",
    "def visualize_error_vs_rho(df, range_label=\"\"):\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    axs[0].scatter(df['rho'], df['rl1'], color='blue', label='L1 Error')\n",
    "    axs[0].set_title(f'L1 Error vs ρ ({range_label})')\n",
    "    axs[0].set_xlabel('ρ')\n",
    "    axs[0].set_ylabel('Relative L1 Error')\n",
    "\n",
    "    axs[1].scatter(df['rho'], df['rl2'], color='green', label='L2 Error')\n",
    "    axs[1].set_title(f'L2 Error vs ρ ({range_label})')\n",
    "    axs[1].set_xlabel('ρ')\n",
    "    axs[1].set_ylabel('Relative L2 Error')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Pearson correlations\n",
    "    p_l1, _ = pearsonr(df['rho'], df['rl1'])\n",
    "    p_l2, _ = pearsonr(df['rho'], df['rl2'])\n",
    "\n",
    "    print(f\"Pearson correlation (ρ vs L1): {p_l1:.3f}\")\n",
    "    print(f\"Pearson correlation (ρ vs L2): {p_l2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage loop over different rho ranges\n",
    "ranges = [\n",
    "    (0.5, 1.0),   # Low variance\n",
    "    (0.5, 4.0),   # Medium variance\n",
    "    (0.5, 10.0),  # High variance\n",
    "]\n",
    "dataset = ODEData(t_range=[0, 1], rho_values=[0.1, 0.3, 0.4], t_points=101, constant_x=1.0, device='cuda:0', use_time_sequencing=True) # here we can use dummy rho values\n",
    "\n",
    "for start, end in ranges:\n",
    "    rho_vals = generate_rho_values(start, end, num=10)\n",
    "    df_results = evaluate_rho_range(rho_vals, dataset, best_model, device='cuda:0')\n",
    "    visualize_error_vs_rho(df_results, range_label=f\"{start}–{end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_error_per_range(ranges, dataset, model, device='cuda:0', points_per_range=10):\n",
    "    avg_errors = []\n",
    "\n",
    "    for start, end in ranges:\n",
    "        rho_values = np.linspace(start, end, points_per_range)\n",
    "        df = evaluate_rho_range(rho_values, dataset, model, device=device)\n",
    "\n",
    "        mean_l1 = df['rl1'].mean()\n",
    "        mean_l2 = df['rl2'].mean()\n",
    "        avg_errors.append({\n",
    "            'range': f\"{start:.1f}–{end:.1f}\",\n",
    "            'mean_l1': mean_l1,\n",
    "            'mean_l2': mean_l2\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(avg_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_errors(df_avg):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot L1 Error\n",
    "    sns.barplot(data=df_avg, x='range', y='mean_l1', ax=axs[0], hue='range', palette=\"Blues_d\", dodge=False)\n",
    "    axs[0].set_title('Average L1 Error per ρ Range')\n",
    "    axs[0].set_ylabel('Mean L1 Error')\n",
    "    axs[0].set_xlabel('ρ Range')\n",
    "    legend = axs[0].get_legend()\n",
    "    if legend:  # Check if the legend exists\n",
    "        legend.remove()\n",
    "\n",
    "    # Plot L2 Error\n",
    "    sns.barplot(data=df_avg, x='range', y='mean_l2', ax=axs[1], hue='range', palette=\"Greens_d\", dodge=False)\n",
    "    axs[1].set_title('Average L2 Error per ρ Range')\n",
    "    axs[1].set_ylabel('Mean L2 Error')\n",
    "    axs[1].set_xlabel('ρ Range')\n",
    "    legend = axs[1].get_legend()\n",
    "    if legend:  # Check if the legend exists\n",
    "        legend.remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [\n",
    "    (-2.0, 1.0),\n",
    "    (0.5, 1.0),\n",
    "    (0.5, 4.0),\n",
    "    (0.5, 10.0),\n",
    "    (-5.0, 20.0)\n",
    "]\n",
    "\n",
    "df_avg = compute_average_error_per_range(ranges, dataset, best_model, device='cuda:0')\n",
    "plot_average_errors(df_avg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
