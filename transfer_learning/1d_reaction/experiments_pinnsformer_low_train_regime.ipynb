{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from torch.optim import LBFGS, Adam\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# make sure that util is correctly accessed from parent directory\n",
    "ppp_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if ppp_dir not in sys.path:\n",
    "    sys.path.insert(0, ppp_dir)\n",
    "\n",
    "from model_parametrized.pinnsformer import PINNsformer\n",
    "from pde_dataset import PDEData\n",
    "from config import HyperparamConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If CUDA is available, print the CUDA version\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    device = 'cpu'\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure initial dataset and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "{'total_i': 10, 'dataset': '1d-reaction-pde', 'model_name': 'pinnsformer', 'in_dim': 3, 'hidden_dim': 256, 'out_dim': 1, 'num_layer': 1, 'init_weights': 'xavier uniform', 'bias_fill': 0.01, 'emb_dim': 32, 'num_heads': 2, 'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 128, 'normalize_res': False, 'normalize_ic': False, 'adaptive_loss_weighting': False}\n"
     ]
    }
   ],
   "source": [
    "config = HyperparamConfig(\n",
    "    total_i=10,\n",
    "    dataset=\"1d-reaction-pde\",\n",
    "    model_name=\"pinnsformer\",  # Model name\n",
    "    in_dim=3,\n",
    "    hidden_dim=256,\n",
    "    out_dim=1,\n",
    "    emb_dim=32,\n",
    "    num_layer=1,\n",
    "    num_heads=2,\n",
    "    init_weights=\"xavier uniform\",\n",
    "    bias_fill=0.01,\n",
    "    optimizer=\"adam\",\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Validate the configuration\n",
    "config.validate()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(config.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hyperparameter Tuning: Small $\\rho$ parameter variance datasets, adam optimizer with mini batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_errors(model, dataset, rho, device):\n",
    "    \n",
    "    # Get test points for the current rho\n",
    "    res_test, x_test, t_test, _ = dataset.get_test_points(rho)\n",
    "    rho_test = torch.full_like(x_test, rho).to(device)  # Create rho tensor for test points\n",
    "\n",
    "    # Compute the analytical solution for the test points\n",
    "    u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(101, 101)\n",
    "\n",
    "    # Model predictions\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "        pred = pred.cpu().detach().numpy().reshape(101, 101)\n",
    "\n",
    "    # Compute relative errors\n",
    "    rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
    "    rl2 = np.sqrt(np.sum((u_analytical - pred) ** 2) / np.sum(u_analytical ** 2))\n",
    "\n",
    "    return rl1, rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# === Helper Functions ===\n",
    "def compute_grad_norm(loss, model):\n",
    "    \"\"\"Compute and normalize gradient of loss w.r.t. model parameters.\"\"\"\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    flat_grad = torch.cat([g.view(-1) for g in grads if g is not None])\n",
    "    return flat_grad / (flat_grad.norm() + 1e-8)\n",
    "\n",
    "def update_momentum(grads, momentum_dict, alpha):\n",
    "    \"\"\"Exponential moving average update for gradient momentum.\"\"\"\n",
    "    for key in grads:\n",
    "        if key not in momentum_dict:\n",
    "            momentum_dict[key] = grads[key].detach().clone()\n",
    "        else:\n",
    "            momentum_dict[key] = alpha * momentum_dict[key] + (1 - alpha) * grads[key].detach()\n",
    "    return momentum_dict\n",
    "\n",
    "def compute_loss_weights(momentum_dict):\n",
    "    \"\"\"Compute inverse-norm weights from gradient momenta.\"\"\"\n",
    "    weights = {k: 1.0 / (v.norm() + 1e-8) for k, v in momentum_dict.items()}\n",
    "    total = sum(weights.values())\n",
    "    return {k: weights[k] / total for k in weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper Function for One Training Run ===\n",
    "def run_training(config_dict, model, optim, dataset, rho_values, device):\n",
    "    \"\"\"\n",
    "    Perform one training run based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        config_dict (dict): Configuration dictionary containing hyperparameters.\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        dataset: Dataset object providing interior and initial condition points.\n",
    "        rho_values (list): List of rho values for training.\n",
    "        device (torch.device): Device to run the training on (e.g., 'cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    wandb.init(\n",
    "        project=\"gnn_1d_reaction\",\n",
    "        config=config_dict,\n",
    "        settings=wandb.Settings(silent=True),\n",
    "        mode=\"offline\"\n",
    "    )\n",
    "\n",
    "    momentum_dict = {}  # Reset for each run\n",
    "    loss_track = {}\n",
    "    model.train() # set to training mode\n",
    "\n",
    "    for i in tqdm(range(config_dict[\"total_i\"])):\n",
    "        total_loss_res = 0.0\n",
    "        total_loss_bc = 0.0\n",
    "        total_loss_ic = 0.0\n",
    "        num_batches = 0  # Track the number of batches\n",
    "\n",
    "        for rho in rho_values:\n",
    "            # Get interior points and create a mini-batch DataLoader\n",
    "            x_res, t_res, rho_res = dataset.get_interior_points(rho)\n",
    "            interior_dataset = TensorDataset(x_res, t_res, rho_res)\n",
    "            interior_loader = DataLoader(interior_dataset, batch_size=config_dict[\"batch_size\"], shuffle=True)\n",
    "\n",
    "            # Get full boundary data for current rho\n",
    "            (x_left, t_left, rho_left,\n",
    "            x_right, t_right, rho_right,\n",
    "            x_upper, t_upper, rho_upper,\n",
    "            x_lower, t_lower, rho_lower) = dataset.get_boundary_points(rho)\n",
    "            u_left, u_right, u_upper, u_lower = dataset.get_boundary_values(rho)\n",
    "\n",
    "            for bx, bt, brho in interior_loader:\n",
    "                bx.requires_grad_()\n",
    "                bt.requires_grad_()\n",
    "\n",
    "                pred_res = model(bx.to(device), bt.to(device), brho.to(device))\n",
    "                u_t = torch.autograd.grad(pred_res, bt.to(device), grad_outputs=torch.ones_like(pred_res),\n",
    "                                          retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "                if config_dict[\"normalize_res\"]:\n",
    "                    normalized_res_error = (u_t - brho * pred_res * (1 - pred_res)) / (\n",
    "                        config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
    "                    loss_res = torch.mean(normalized_res_error ** 2)\n",
    "                else:\n",
    "                    loss_res = torch.mean((u_t - brho * pred_res * (1 - pred_res)) ** 2)\n",
    "\n",
    "                pred_ic = model(x_lower.to(device), t_lower.to(device), rho_lower.to(device))\n",
    "\n",
    "                if config_dict[\"normalize_ic\"]:\n",
    "                    normalized_ic_error = (pred_ic - u_lower.to(device)) / (\n",
    "                        config_dict[\"alpha\"] * torch.sqrt(brho) + config_dict[\"epsilon\"])\n",
    "                    loss_ic = torch.mean(normalized_ic_error ** 2)\n",
    "                else:\n",
    "                    loss_ic = torch.mean((pred_ic - u_lower.to(device)) ** 2)\n",
    "\n",
    "                if config_dict[\"adaptive_loss_weighting\"]:\n",
    "                    grad_res = compute_grad_norm(loss_res, model)\n",
    "                    grad_ic = compute_grad_norm(loss_ic, model)\n",
    "\n",
    "                    grads = {'res': grad_res, 'ic': grad_ic}\n",
    "                    momentum_dict = update_momentum(grads, momentum_dict, config_dict[\"adaptive_loss_coeff\"])\n",
    "                    gamma = compute_loss_weights(momentum_dict)\n",
    "\n",
    "                    loss = gamma['res'] * loss_res + gamma['ic'] * loss_ic\n",
    "                else:\n",
    "                    loss = loss_res + loss_ic\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optim.step()\n",
    "\n",
    "                total_loss_res += loss_res.item()\n",
    "                total_loss_ic += loss_ic.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        avg_loss_res = total_loss_res / num_batches\n",
    "        avg_loss_ic = total_loss_ic / num_batches\n",
    "        avg_total_loss = avg_loss_res + avg_loss_ic\n",
    "\n",
    "        wandb_dict = {\n",
    "            \"iteration\": i,\n",
    "            \"avg_loss_res\": avg_loss_res,\n",
    "            \"avg_loss_ic\": avg_loss_ic,\n",
    "            \"avg_total_loss\": avg_total_loss\n",
    "        }\n",
    "\n",
    "        total_l1 = 0.0\n",
    "        total_l2 = 0.0\n",
    "        for rho in rho_values:\n",
    "            rl1, rl2 = compute_relative_errors(model, dataset, rho, device)\n",
    "            wandb_dict[f\"{rho}_rl1\"] = rl1\n",
    "            wandb_dict[f\"{rho}_rl2\"] = rl2\n",
    "            total_l1 += rl1 \n",
    "            total_l2 += rl2\n",
    "\n",
    "        average_l1 = total_l1 / len(rho_values) \n",
    "        average_l2 = total_l2 / len(rho_values)\n",
    "\n",
    "        wandb.log(wandb_dict)\n",
    "        loss_track[i] = wandb_dict\n",
    "\n",
    "    wandb.finish()\n",
    "    return loss_track, average_l1, average_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(config.bias_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a low parameter regime (0.5-1) to train the model. This is not a representative range, but still interesting to explore. See `1d_logistic_ode_regimes.ipynb` for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rho_values(start, end, num=10):\n",
    "    return np.linspace(start, end, num).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:29<00:00,  7.78s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [09:52<04:51,  8.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     83\u001b[0m     optim \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 84\u001b[0m loss_track, avg_l1, avg_l2 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Update the best model if the current one is better\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_l1 \u001b[38;5;241m<\u001b[39m best_l1 \u001b[38;5;129;01mand\u001b[39;00m avg_l2 \u001b[38;5;241m<\u001b[39m best_l2:\n",
      "Cell \u001b[0;32mIn[24], line 69\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(config_dict, model, optim, dataset, rho_values, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madaptive_loss_weighting\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     68\u001b[0m     grad_res \u001b[38;5;241m=\u001b[39m compute_grad_norm(loss_res, model)\n\u001b[0;32m---> 69\u001b[0m     grad_ic \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_grad_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_ic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres\u001b[39m\u001b[38;5;124m'\u001b[39m: grad_res, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic\u001b[39m\u001b[38;5;124m'\u001b[39m: grad_ic}\n\u001b[1;32m     72\u001b[0m     momentum_dict \u001b[38;5;241m=\u001b[39m update_momentum(grads, momentum_dict, config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madaptive_loss_coeff\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m, in \u001b[0;36mcompute_grad_norm\u001b[0;34m(loss, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_grad_norm\u001b[39m(loss, model):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute and normalize gradient of loss w.r.t. model parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([g\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_grad \u001b[38;5;241m/\u001b[39m (flat_grad\u001b[38;5;241m.\u001b[39mnorm() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:436\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    432\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    433\u001b[0m         grad_outputs_\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    447\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    449\u001b[0m     ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define rho values\n",
    "rho_values = [0.5, 0.7, 0.8, 1.0] # small range\n",
    "\n",
    "\"\"\" start = 0.5 \n",
    "end = 4.0\n",
    "rho_values = generate_rho_values(start, end, num=5) \"\"\"\n",
    "\n",
    "\n",
    "# Create the dataset using the ODEData class.\n",
    "# Note: We now provide t_range, t_points, a constant x value (e.g., 1.0), and rho_values.\n",
    "dataset = PDEData(x_range=[0, 2*np.pi],t_range=[0, 1], rho_values=rho_values,x_points=101 ,t_points=101)\n",
    "\n",
    "device=\"cpu\"\n",
    "# === Variations ===\n",
    "normalize_res_values = [False] #[True, False]\n",
    "normalize_ic_values = [False] #[True, False]\n",
    "adaptive_loss = [True, False]\n",
    "adaptive_loss_weighting_coeffs = [0.7, 0.9]\n",
    "alpha_values = [None]#[0.1, 0.25] #[0.1, 0.25, 0.5]\n",
    "epsilon_values = [None]#[0.1, 0.3] #[0.1, 0.5, 1.0]\n",
    "iteration_steps = [50,100,400] #[50, 100, 400]\n",
    "models = []\n",
    "\n",
    "\"\"\" \n",
    "for init_act,subseq_act in [(\"sin\", \"tanh\"), (\"sin\",\"gelu\"), (\"tanh\", \"gelu\")]:\n",
    "    model = PINNff(in_dim=config.in_dim, \n",
    "                   hidden_dim=config.hidden_dim,\n",
    "                   out_dim=config.out_dim,\n",
    "                   num_layer=config.num_layer,\n",
    "                   init_act_func=init_act,\n",
    "                   subseq_activ_func=subseq_act).to(device)\n",
    "    model.apply(init_weights)\n",
    "    models.append((model,init_act, subseq_act)) \"\"\"\n",
    "\n",
    "model_variations = [\n",
    "    (32,1,128),\n",
    "    (32,2,256),\n",
    "    (33,3,512),\n",
    "    (66,3,512)\n",
    "]\n",
    "for emb_dim,num_heads,hidden_dim in model_variations:\n",
    "    model = PINNsformer(\n",
    "            d_out=config.out_dim,\n",
    "            d_model=emb_dim,\n",
    "            d_hidden=hidden_dim,\n",
    "            N=config.num_layer,\n",
    "            heads=num_heads).to(device)\n",
    "    model.apply(init_weights)\n",
    "    models.append((model, emb_dim, num_heads, hidden_dim))\n",
    "\n",
    "\n",
    "# === Grid Search ===\n",
    "best_model = None\n",
    "best_config = None\n",
    "best_l1 = float('inf')\n",
    "best_l2 = float('inf')\n",
    "\n",
    "for model,emb_dim,num_heads,hidden_dim in models: \n",
    "    for normalize_res in normalize_res_values:\n",
    "        for normalize_ic in normalize_ic_values:\n",
    "            for ad_loss in adaptive_loss:\n",
    "                for a_coeff in adaptive_loss_weighting_coeffs:\n",
    "                    if ad_loss == False:\n",
    "                        continue\n",
    "                    for alpha in alpha_values:\n",
    "                        for epsilon in epsilon_values:\n",
    "                            for total_i in iteration_steps:\n",
    "                                config.normalize_res = normalize_res\n",
    "                                config.normalize_ic = normalize_ic\n",
    "                                config.adaptive_loss_coeff = a_coeff\n",
    "                                config.alpha = alpha\n",
    "                                config.epsilon = epsilon\n",
    "                                config.total_i = total_i\n",
    "                                config.adaptive_loss_weighting = ad_loss\n",
    "                                config.hidden_dim = hidden_dim \n",
    "                                config.emb_dim = emb_dim\n",
    "                                config.num_heads = num_heads\n",
    "                                config.validate()\n",
    "\n",
    "                                # Call the training function\n",
    "                                config_dict = config.to_dict()\n",
    "                                if config.optimizer == \"adam\":\n",
    "                                    optim = Adam(model.parameters(), lr=config_dict[\"learning_rate\"])\n",
    "                                loss_track, avg_l1, avg_l2 = run_training(config_dict, model, optim, dataset, rho_values, device)\n",
    "\n",
    "                                # Update the best model if the current one is better\n",
    "                                if avg_l1 < best_l1 and avg_l2 < best_l2:\n",
    "                                    best_model = model\n",
    "                                    best_config = config_dict.copy()\n",
    "                                    best_l1 = avg_l1\n",
    "                                    best_l2 = avg_l2\n",
    "\n",
    "# Print the best model and configuration\n",
    "print(\"Best Model Configuration:\")\n",
    "print(best_config)\n",
    "print(f\"Best Average L1 Error: {best_l1}\")\n",
    "print(f\"Best Average L2 Error: {best_l2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\".\")  # Base directory for results\n",
    "images_dir = base_dir / \"images\"  # Subdirectory for images\n",
    "weights_dir = base_dir / \"weights\"  # Subdirectory for stored model\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "weights_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_and_errors(rho_values, predictions, analytical_solutions, errors, num_cols=4, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot predictions and errors for multiple rho values in a grid layout.\n",
    "\n",
    "    Args:\n",
    "        rho_values (list): List of rho values.\n",
    "        predictions (dict): Dictionary of predictions for each rho value.\n",
    "        analytical_solutions (dict): Dictionary of analytical solutions for each rho value.\n",
    "        errors (dict): Dictionary of absolute errors for each rho value.\n",
    "        num_cols (int): Number of columns in the grid (default: 4).\n",
    "        save_path (str or Path, optional): Path to save the figure. If None, the figure is not saved.\n",
    "    \"\"\"\n",
    "    num_rho = len(rho_values)\n",
    "    num_rows = 2  # Fixed: Row 1 for predictions, Row 2 for errors\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 6))\n",
    "\n",
    "    # Ensure axes is a 2D array even if num_cols == 1\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for idx, rho in enumerate(rho_values):\n",
    "        col = idx % num_cols\n",
    "\n",
    "        # Extract data for the current rho\n",
    "        pred = predictions[rho]\n",
    "        analytical = analytical_solutions[rho]\n",
    "        abs_error = errors[rho]\n",
    "        percentage_error = (abs_error / np.maximum(analytical, 1e-8)) * 100  # Avoid division by zero\n",
    "\n",
    "        ax_pred = axes[0, col]\n",
    "        ax_pred.plot(pred, label=\"Prediction\", color=\"blue\", linewidth=2)\n",
    "        ax_pred.plot(analytical, label=\"Analytical\", color=\"orange\", linestyle=\"dashed\", linewidth=2)\n",
    "        ax_pred.set_title(f\"Rho: {rho} - Prediction\")\n",
    "        ax_pred.set_xlabel(\"t - Time\")  # Horizontal axis label\n",
    "        ax_pred.set_ylabel(\"u(t) - Value\")  # Vertical axis label\n",
    "        ax_pred.legend()\n",
    "        \n",
    "\n",
    "        # Plot absolute and percentage errors (Row 2)\n",
    "        ax_err = axes[1, col]\n",
    "        #ax_err.plot(abs_error, label=\"Absolute Error\", color=\"red\", linewidth=2)\n",
    "        ax_err.plot(percentage_error, label=\"Percentage Error\", color=\"green\", linestyle=\"dotted\", linewidth=2)\n",
    "        ax_err.set_title(f\"Rho: {rho} - Relative Error (%)\")\n",
    "        ax_err.set_xlabel(\"t - Time\")  # Horizontal axis label\n",
    "        ax_err.set_ylabel(\"delta u(t) (%)\")  # Vertical axis label\n",
    "        ax_err.legend()\n",
    "\n",
    "    # Hide unused subplots if num_rho < num_cols\n",
    "    for idx in range(num_rho, num_cols):\n",
    "        axes[0, idx].axis(\"off\")\n",
    "        axes[1, idx].axis(\"off\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure if a save path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_parameter_range(rho_values, dataset, model):\n",
    "    predictions = {}\n",
    "    analytical_solutions = {}\n",
    "    errors = {}\n",
    "\n",
    "    for rho in rho_values:\n",
    "        # Get test points for the current rho\n",
    "        x_test, t_test, _ = dataset.get_test_points(rho)\n",
    "        rho_test = torch.full_like(x_test, rho).to(device)\n",
    "\n",
    "        # Compute analytical solution\n",
    "        u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        # Compute best model predictions\n",
    "        with torch.no_grad():\n",
    "            pred = best_model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "            pred = pred.cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        # Compute error\n",
    "        error = np.abs(u_analytical - pred)\n",
    "\n",
    "        # Store results\n",
    "        predictions[rho] = pred\n",
    "        analytical_solutions[rho] = u_analytical\n",
    "        errors[rho] = error\n",
    "\n",
    "    return predictions, analytical_solutions, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "predictions, analytical_solutions, errors = test_on_parameter_range(rho_values, dataset, best_model)\n",
    "\n",
    "# Plot predictions and errors\n",
    "plot_predictions_and_errors(\n",
    "    rho_values=rho_values,\n",
    "    predictions=predictions,\n",
    "    analytical_solutions=analytical_solutions,\n",
    "    errors=errors,\n",
    "    num_cols=4,\n",
    "    save_path=images_dir / \"predictions_and_errors.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing best model on different rho ranges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model configuration:**\n",
    "\n",
    "{'total_i': 400, 'dataset': '1d-logistic-ode', 'model_name': 'pinn_ff', 'in_dim': 3, 'hidden_dim': 256, 'out_dim': 1, 'num_layer': 4, 'init_weights': 'xavier uniform', 'bias_fill': 0.01, 'init_activ_func': 'tanh', 'subseq_activ_func': 'gelu', 'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 128, 'normalize_res': False, 'normalize_ic': False, 'adaptive_loss_weighting': True, 'adaptive_loss_coeff': 0.9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_model = False\n",
    "\n",
    "if load_existing_model == True:\n",
    "    best_model = PINNff(\n",
    "        in_dim=3,  # Input dimension (e.g., x, t, rho)\n",
    "        hidden_dim=256,  # Hidden layer size (must match the saved model)\n",
    "        out_dim=1,  # Output dimension\n",
    "        num_layer=4,  # Number of layers\n",
    "        init_act_func=\"tanh\",  # Initial activation function\n",
    "        subseq_activ_func=\"gelu\"  # Subsequent activation function\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the model weights\n",
    "    model_path = weights_dir / \"1d_logistic_ode_fls_extended.pt\"  # Path to the saved model\n",
    "    best_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    best_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Function 1: Generate uniformly spaced ρ values in a given range\n",
    "def generate_rho_values(start, end, num=10):\n",
    "    return np.linspace(start, end, num).tolist()\n",
    "\n",
    "# Function 2: Evaluate model on ρ values and return error stats\n",
    "def evaluate_rho_range(rho_values, dataset, model, device='cpu'):\n",
    "    results = []\n",
    "\n",
    "    for rho in rho_values:\n",
    "        x_test, t_test, _ = dataset.get_interior_input_without_points()\n",
    "        rho_test = torch.full_like(x_test, rho).to(device)\n",
    "\n",
    "        u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0]\n",
    "            pred = pred.cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "        rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
    "        rl2 = np.sqrt(np.sum((u_analytical - pred) ** 2) / np.sum(u_analytical ** 2))\n",
    "\n",
    "        results.append({'rho': rho, 'rl1': rl1, 'rl2': rl2})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Function 3: Plot scatter and compute Pearson correlation\n",
    "def visualize_error_vs_rho(df, range_label=\"\"):\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    axs[0].scatter(df['rho'], df['rl1'], color='blue', label='L1 Error')\n",
    "    axs[0].set_title(f'L1 Error vs ρ ({range_label})')\n",
    "    axs[0].set_xlabel('ρ')\n",
    "    axs[0].set_ylabel('Relative L1 Error')\n",
    "\n",
    "    axs[1].scatter(df['rho'], df['rl2'], color='green', label='L2 Error')\n",
    "    axs[1].set_title(f'L2 Error vs ρ ({range_label})')\n",
    "    axs[1].set_xlabel('ρ')\n",
    "    axs[1].set_ylabel('Relative L2 Error')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Pearson correlations\n",
    "    p_l1, _ = pearsonr(df['rho'], df['rl1'])\n",
    "    p_l2, _ = pearsonr(df['rho'], df['rl2'])\n",
    "\n",
    "    print(f\"Pearson correlation (ρ vs L1): {p_l1:.3f}\")\n",
    "    print(f\"Pearson correlation (ρ vs L2): {p_l2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage loop over different rho ranges\n",
    "ranges = [\n",
    "    (0.5, 1.0),   # Low variance\n",
    "    (0.5, 4.0),   # Medium variance\n",
    "    (0.5, 10.0),  # High variance\n",
    "]\n",
    "dataset = ODEData(t_range=[0, 1], rho_values=[0.1, 0.3, 0.4], t_points=101, constant_x=1.0, device='cuda:0') # here we can use dummy rho values\n",
    "\n",
    "for start, end in ranges:\n",
    "    rho_vals = generate_rho_values(start, end, num=10)\n",
    "    df_results = evaluate_rho_range(rho_vals, dataset, best_model, device='cuda:0')\n",
    "    visualize_error_vs_rho(df_results, range_label=f\"{start}–{end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Analysis Across ρ Ranges\n",
    "\n",
    "We evaluated how relative errors (L1 and L2) evolve as the parameter ρ varies in the logistic growth ODE. The model was tested on ρ values from three different intervals:\n",
    "\n",
    "- **Range 1:** [0.5, 1.0] — low variance\n",
    "- **Range 2:** [0.5, 4.0] — medium variance\n",
    "- **Range 3:** [0.5, 10.0] — high variance\n",
    "\n",
    "For each range, we computed the Pearson correlation between ρ and both relative L1 and L2 errors. Here's what we observed:\n",
    "\n",
    "#### Error vs. ρ Summary\n",
    "\n",
    "| ρ Range       | Pearson (ρ vs L1) | Pearson (ρ vs L2) | Interpretation                                                   |\n",
    "|---------------|-------------------|-------------------|------------------------------------------------------------------|\n",
    "| [0.5 – 1.0]    | 0.024             | -0.078            | 🔹 Model performs consistently; generalizes well in this regime. |\n",
    "| [0.5 – 4.0]    | 0.980             | 0.980             | ⚠️ Error increases sharply with ρ — model fails to generalize.   |\n",
    "| [0.5 – 10.0]   | 0.830             | 0.851             | ⚠️ Error saturates; model can't handle sharp solutions for high ρ.|\n",
    "\n",
    "#### Key Insights\n",
    "\n",
    "- The PINN generalizes well over **low ρ ranges**.\n",
    "- As ρ increases, the logistic ODE solution becomes steeper (stiffer), and the model fails to represent it.\n",
    "- Beyond a certain point, the error **saturates**, suggesting the model hits a capacity wall or lacks proper expressiveness.\n",
    "\n",
    "Next, we compute and visualize the **average error per range** to compare difficulty levels of different ρ intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_error_per_range(ranges, dataset, model, device='cuda:0', points_per_range=10):\n",
    "    avg_errors = []\n",
    "\n",
    "    for start, end in ranges:\n",
    "        rho_values = np.linspace(start, end, points_per_range)\n",
    "        df = evaluate_rho_range(rho_values, dataset, model, device=device)\n",
    "\n",
    "        mean_l1 = df['rl1'].mean()\n",
    "        mean_l2 = df['rl2'].mean()\n",
    "        avg_errors.append({\n",
    "            'range': f\"{start:.1f}–{end:.1f}\",\n",
    "            'mean_l1': mean_l1,\n",
    "            'mean_l2': mean_l2\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(avg_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_errors(df_avg):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot L1 Error\n",
    "    sns.barplot(data=df_avg, x='range', y='mean_l1', ax=axs[0], hue='range', palette=\"Blues_d\", dodge=False)\n",
    "    axs[0].set_title('Average L1 Error per ρ Range')\n",
    "    axs[0].set_ylabel('Mean L1 Error')\n",
    "    axs[0].set_xlabel('ρ Range')\n",
    "    legend = axs[0].get_legend()\n",
    "    if legend:  # Check if the legend exists\n",
    "        legend.remove()\n",
    "\n",
    "    # Plot L2 Error\n",
    "    sns.barplot(data=df_avg, x='range', y='mean_l2', ax=axs[1], hue='range', palette=\"Greens_d\", dodge=False)\n",
    "    axs[1].set_title('Average L2 Error per ρ Range')\n",
    "    axs[1].set_ylabel('Mean L2 Error')\n",
    "    axs[1].set_xlabel('ρ Range')\n",
    "    legend = axs[1].get_legend()\n",
    "    if legend:  # Check if the legend exists\n",
    "        legend.remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = [\n",
    "    (-2.0, 1.0),\n",
    "    (0.5, 1.0),\n",
    "    (0.5, 4.0),\n",
    "    (0.5, 10.0),\n",
    "    (-5.0, 20.0)\n",
    "]\n",
    "\n",
    "df_avg = compute_average_error_per_range(ranges, dataset, best_model, device='cuda:0')\n",
    "plot_average_errors(df_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Error Comparison Across ρ Ranges\n",
    "\n",
    "To further summarize the model's performance across different parameter regimes, we computed the **mean relative L1 and L2 errors** over uniformly spaced ρ values within each range.\n",
    "\n",
    "The bar plots below show the average errors for each ρ interval:\n",
    "\n",
    "- Very **narrow ranges** (e.g. `[0.5–1.0]`) result in **low and stable error**.\n",
    "- As the range grows and includes **larger ρ values**, both L1 and L2 errors increase.\n",
    "- In the extreme case (e.g. `[-5.0–20.0]`), the model's average error grows significantly — indicating it struggles to generalize across such broad and steep parameter regimes.\n",
    "\n",
    "This view complements the correlation plots by showing the **absolute difficulty** of each regime, not just the trend.\n",
    "This confirms that **parameter range width** and **solution stiffness** are major factors influencing PINN generalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
