{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import random\n",
    "from torch.optim import LBFGS, Adam\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# make sure that util is correctly accessed from parent directory\n",
    "ppp_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if ppp_dir not in sys.path:\n",
    "    sys.path.insert(0, ppp_dir)\n",
    "\n",
    "from util import *\n",
    "from model.pinn import PINNs\n",
    "from model.pinnsformer import PINNsformer, PINNsformer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "Number of CUDA devices: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If CUDA is available, print the CUDA version\n",
    "if cuda_available:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds and configure cuda device\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDEData(Dataset):\n",
    "    def __init__(self, x_range, t_range, rho_values, x_points, t_points):\n",
    "        \"\"\"\n",
    "        Initialize the dataset for PDE data with multiple rho values.\n",
    "\n",
    "        Args:\n",
    "            x_range (list): Spatial domain [x_min, x_max].\n",
    "            t_range (list): Temporal domain [t_min, t_max].\n",
    "            rho_values (list): List of rho values for different scenarios.\n",
    "            x_points (int): Number of points in the spatial domain.\n",
    "            t_points (int): Number of points in the temporal domain.\n",
    "            device (str): Device to store the tensors ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.device = \"cuda:0\"\n",
    "        self.x_range = x_range\n",
    "        self.t_range = t_range\n",
    "        self.rho_values = rho_values  # Store multiple rho values\n",
    "        self.x_points = x_points\n",
    "        self.t_points = t_points\n",
    "        \n",
    "        # Generate the data for all rho values\n",
    "        self.data = {}\n",
    "        for rho in rho_values:\n",
    "            res, b_left, b_right, b_upper, b_lower = self._generate_data()\n",
    "            \n",
    "            # Convert boundary points to PyTorch tensors\n",
    "            b_left = torch.tensor(b_left, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            b_right = torch.tensor(b_right, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            b_upper = torch.tensor(b_upper, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            b_lower = torch.tensor(b_lower, dtype=torch.float32, requires_grad=True).to(self.device)\n",
    "            \n",
    "            self.data[rho] = {\n",
    "                'res': torch.tensor(res, dtype=torch.float32, requires_grad=True).to(self.device),\n",
    "                'b_left': b_left,\n",
    "                'b_right': b_right,\n",
    "                'b_upper': b_upper,\n",
    "                'b_lower': b_lower,\n",
    "                # Precompute analytical solutions for boundary points\n",
    "                'u_left': self.analytical_solution(b_left[:, 0:1], b_left[:, 1:2], rho),\n",
    "                'u_right': self.analytical_solution(b_right[:, 0:1], b_right[:, 1:2], rho),\n",
    "                'u_upper': self.analytical_solution(b_upper[:, 0:1], b_upper[:, 1:2], rho),\n",
    "                'u_lower': self.analytical_solution(b_lower[:, 0:1], b_lower[:, 1:2], rho),\n",
    "            }\n",
    "    \n",
    "    def _generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate the interior and boundary points for the PDE.\n",
    "\n",
    "        Returns:\n",
    "            res (np.ndarray): Interior points.\n",
    "            b_left, b_right, b_upper, b_lower (np.ndarray): Boundary points.\n",
    "        \"\"\"\n",
    "        x = np.linspace(self.x_range[0], self.x_range[1], self.x_points)\n",
    "        t = np.linspace(self.t_range[0], self.t_range[1], self.t_points)\n",
    "        \n",
    "        x_mesh, t_mesh = np.meshgrid(x, t)\n",
    "        data = np.concatenate((np.expand_dims(x_mesh, -1), np.expand_dims(t_mesh, -1)), axis=-1)\n",
    "        \n",
    "        b_left = data[0, :, :] \n",
    "        b_right = data[-1, :, :]\n",
    "        b_upper = data[:, -1, :]\n",
    "        b_lower = data[:, 0, :]\n",
    "        res = data.reshape(-1, 2)\n",
    "\n",
    "        return res, b_left, b_right, b_upper, b_lower\n",
    "    \n",
    "    def analytical_solution(self, x, t, rho):\n",
    "        \"\"\"\n",
    "        Compute the analytical solution u_ana(x, t, rho).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Spatial points.\n",
    "            t (torch.Tensor): Temporal points.\n",
    "            rho (float): Reaction coefficient.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Analytical solution u(x, t, rho).\n",
    "        \"\"\"\n",
    "        h = torch.exp(- (x - torch.pi)**2 / (2 * (torch.pi / 4)**2))\n",
    "        return h * torch.exp(rho * t) / (h * torch.exp(rho * t) + 1 - h)\n",
    "    \n",
    "    def get_interior_points(self, rho):\n",
    "        \"\"\"\n",
    "        Get the interior points (x_res, t_res, rho_res) for a specific rho.\n",
    "\n",
    "        Args:\n",
    "            rho (float): The rho value for the current scenario.\n",
    "\n",
    "        Returns:\n",
    "            x_res, t_res, rho_res (torch.Tensor): Interior points with rho values.\n",
    "        \"\"\"\n",
    "        res = self.data[rho]['res']\n",
    "        x_res, t_res = res[:, 0:1], res[:, 1:2]\n",
    "        rho_res = torch.full_like(x_res, rho)  # Same shape, constant rho\n",
    "        return x_res, t_res, rho_res\n",
    "    \n",
    "    def get_boundary_points(self, rho):\n",
    "        \"\"\"\n",
    "        Get the boundary points (x_left, t_left, etc.) for a specific rho.\n",
    "\n",
    "        Args:\n",
    "            rho (float): The rho value for the current scenario.\n",
    "\n",
    "        Returns:\n",
    "            Boundary points (torch.Tensor): x, t, and rho values for all boundaries.\n",
    "        \"\"\"\n",
    "        b_left = self.data[rho]['b_left']\n",
    "        b_right = self.data[rho]['b_right']\n",
    "        b_upper = self.data[rho]['b_upper']\n",
    "        b_lower = self.data[rho]['b_lower']\n",
    "        \n",
    "        x_left, t_left = b_left[:, 0:1], b_left[:, 1:2]\n",
    "        x_right, t_right = b_right[:, 0:1], b_right[:, 1:2]\n",
    "        x_upper, t_upper = b_upper[:, 0:1], b_upper[:, 1:2]\n",
    "        x_lower, t_lower = b_lower[:, 0:1], b_lower[:, 1:2]\n",
    "        \n",
    "        rho_left = torch.full_like(x_left, rho)\n",
    "        rho_right = torch.full_like(x_right, rho)\n",
    "        rho_upper = torch.full_like(x_upper, rho)\n",
    "        rho_lower = torch.full_like(x_lower, rho)\n",
    "        \n",
    "        return x_left, t_left, rho_left, x_right, t_right, rho_right, x_upper, t_upper, rho_upper, x_lower, t_lower, rho_lower\n",
    "    \n",
    "    def get_boundary_values(self, rho):\n",
    "        \"\"\"\n",
    "        Get the precomputed analytical solutions for the boundary points.\n",
    "\n",
    "        Args:\n",
    "            rho (float): The rho value for the current scenario.\n",
    "\n",
    "        Returns:\n",
    "            u_left, u_right, u_upper, u_lower (torch.Tensor): Analytical solutions at the boundaries.\n",
    "        \"\"\"\n",
    "        return (self.data[rho]['u_left'], self.data[rho]['u_right'], \n",
    "                self.data[rho]['u_upper'], self.data[rho]['u_lower'])\n",
    "    \n",
    "    def get_test_points(self, rho):\n",
    "        \"\"\"\n",
    "        Get the test points (res_test) and their spatial and temporal components for a specific rho.\n",
    "\n",
    "        Args:\n",
    "            rho (float): The rho value for the current scenario.\n",
    "\n",
    "        Returns:\n",
    "            res_test (torch.Tensor): Test points as a tensor.\n",
    "            x_test, t_test, rho_test (torch.Tensor): Spatial, temporal, and rho components of the test points.\n",
    "        \"\"\"\n",
    "        res_test = self.data[rho]['res']\n",
    "        x_test, t_test = res_test[:, 0:1], res_test[:, 1:2]\n",
    "        rho_test = torch.full_like(x_test, rho)  # Same shape, constant rho\n",
    "        return res_test, x_test, t_test, rho_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Configuration parameters and wandb logging\n",
    "total_i = 50\n",
    "model_name = \"pinnsformer_params\"\n",
    "config_dict = {\n",
    "    \"total_i\": total_i,\n",
    "    \"model\": model_name,\n",
    "    \"d_out\": 1,\n",
    "    \"d_model\": 32,\n",
    "    \"d_hidden\": 512,\n",
    "    \"N\": 1,\n",
    "    \"heads\": 2,\n",
    "    \"bias_fill\": 0.01,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 1e-3\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"pinnsformer\",\n",
    "    config=config_dict,\n",
    "    settings=wandb.Settings(silent=True)\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Weight initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(config_dict[\"bias_fill\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINNsformer_params(\n",
      "  (linear_emb): Linear(in_features=3, out_features=32, bias=True)\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear): Sequential(\n",
      "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "            (1): WaveAct()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): WaveAct()\n",
      "            (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (act1): WaveAct()\n",
      "        (act2): WaveAct()\n",
      "      )\n",
      "    )\n",
      "    (act): WaveAct()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear): Sequential(\n",
      "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "            (1): WaveAct()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): WaveAct()\n",
      "            (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (act1): WaveAct()\n",
      "        (act2): WaveAct()\n",
      "      )\n",
      "    )\n",
      "    (act): WaveAct()\n",
      "  )\n",
      "  (linear_out): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): WaveAct()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): WaveAct()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "453593\n"
     ]
    }
   ],
   "source": [
    "model = PINNsformer_params(\n",
    "    d_out=config_dict[\"d_out\"],\n",
    "    d_model=config_dict[\"d_model\"],\n",
    "    d_hidden=config_dict[\"d_hidden\"],\n",
    "    N=config_dict[\"N\"],\n",
    "    heads=config_dict[\"heads\"]\n",
    ").to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "if config_dict[\"optimizer\"] == \"LBFGS\":\n",
    "    optim = LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "elif config_dict[\"optimizer\"] == \"adam\":\n",
    "    optim = Adam(model.parameters(), lr=config_dict[\"learning_rate\"])\n",
    "\n",
    "print(model)\n",
    "print(get_n_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" rho_values = [5.0]\n",
    "dataset = PDEData(x_range=[0, 2 * np.pi], t_range=[0, 1], rho_values=rho_values, x_points=101, t_points=101)\n",
    "\n",
    "# %%\n",
    "# Training loop\n",
    "loss_track = {i: {} for i in range(total_i)}\n",
    "\n",
    "for i in tqdm(range(total_i)):\n",
    "    for rho in rho_values:\n",
    "        def closure():\n",
    "            # Get interior points\n",
    "            x_res, t_res, rho_res = dataset.get_interior_points(rho)\n",
    "            # Get boundary points and their analytical values\n",
    "            x_left, t_left, rho_left, x_right, t_right, rho_right, x_upper, t_upper, rho_upper, x_lower, t_lower, rho_lower = dataset.get_boundary_points(rho)\n",
    "            u_left, u_right, u_upper, u_lower = dataset.get_boundary_values(rho)\n",
    "            \n",
    "            # Model predictions for interior and boundary points\n",
    "            pred_res = model(x_res, t_res, rho_res)\n",
    "            pred_left = model(x_left, t_left, rho_left)\n",
    "            pred_right = model(x_right, t_right, rho_right)\n",
    "            pred_upper = model(x_upper, t_upper, rho_upper)\n",
    "            pred_lower = model(x_lower, t_lower, rho_lower)\n",
    "\n",
    "            # Compute derivatives with respect to space and time\n",
    "            u_x = torch.autograd.grad(pred_res, x_res, grad_outputs=torch.ones_like(pred_res),\n",
    "                                        retain_graph=True, create_graph=True)[0]\n",
    "            u_t = torch.autograd.grad(pred_res, t_res, grad_outputs=torch.ones_like(pred_res),\n",
    "                                        retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "            # Define the loss terms:\n",
    "            # PDE residual loss (here we assume u_t - rho * u * (1-u) = 0, adjust as needed)\n",
    "            loss_res = torch.mean((u_t - rho * pred_res * (1 - pred_res)) ** 2)\n",
    "            # Boundary condition loss (e.g., matching the solution at t=t_min vs t=t_max)\n",
    "            loss_bc = torch.mean((pred_upper - pred_lower) ** 2)\n",
    "            # Initial condition loss (here comparing the left boundary with the analytical solution)\n",
    "            loss_ic = torch.mean((pred_left[:,0] - u_left[:,0]) ** 2)\n",
    "\n",
    "            loss = loss_res + loss_bc + loss_ic\n",
    "\n",
    "            wandb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"loss_res\": loss_res.item(),\n",
    "                \"loss_bc\": loss_bc.item(),\n",
    "                \"loss_ic\": loss_ic.item(),\n",
    "                \"iteration\": i,\n",
    "                \"rho\": rho\n",
    "            })\n",
    "\n",
    "            # Track losses in a dictionary\n",
    "            if rho not in loss_track[i]:\n",
    "                loss_track[i][rho] = {}\n",
    "            loss_track[i][rho][\"loss_res\"] = loss_res.item()\n",
    "            loss_track[i][rho][\"loss_bc\"] = loss_bc.item()\n",
    "            loss_track[i][rho][\"loss_ic\"] = loss_ic.item()\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss\n",
    "\n",
    "        optim.step(closure)\n",
    "\n",
    "wandb.finish() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m             loss.backward()\n\u001b[32m     54\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m         loss_val = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m         wandb.log({\n\u001b[32m     58\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: epoch,\n\u001b[32m     59\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrho\u001b[39m\u001b[33m\"\u001b[39m: rho,\n\u001b[32m     60\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: loss_val.item()\n\u001b[32m     61\u001b[39m         })\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m complete, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prdie\\OneDrive\\Sources\\pinnsformer\\gnn_project\\gnn_project_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prdie\\OneDrive\\Sources\\pinnsformer\\gnn_project\\gnn_project_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prdie\\OneDrive\\Sources\\pinnsformer\\gnn_project\\gnn_project_env\\Lib\\site-packages\\torch\\optim\\adam.py:202\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    205\u001b[39m     params_with_grad: List[Tensor] = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mclosure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     50\u001b[39m loss_ic = torch.mean((pred_left[:, \u001b[32m0\u001b[39m] - u_left[:, \u001b[32m0\u001b[39m]) ** \u001b[32m2\u001b[39m)\n\u001b[32m     52\u001b[39m loss = loss_res + loss_bc + loss_ic\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prdie\\OneDrive\\Sources\\pinnsformer\\gnn_project\\gnn_project_env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prdie\\OneDrive\\Sources\\pinnsformer\\gnn_project\\gnn_project_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prdie\\OneDrive\\Sources\\pinnsformer\\gnn_project\\gnn_project_env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "rho_values = [5.0]\n",
    "dataset = PDEData(x_range=[0, 2 * np.pi], t_range=[0, 1], rho_values=rho_values, x_points=101, t_points=101)\n",
    "\n",
    "# %%\n",
    "# Training loop\n",
    "loss_track = {i: {} for i in range(total_i)}\n",
    "\n",
    "# Set up Adam optimizer\n",
    "\n",
    "batch_size = config_dict[\"batch_size\"]\n",
    "num_epochs = total_i  # total_i from your config\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for rho in rho_values:\n",
    "        # Get interior points and create a mini-batch DataLoader\n",
    "        x_res, t_res, rho_res = dataset.get_interior_points(rho)\n",
    "        interior_dataset = TensorDataset(x_res, t_res, rho_res)\n",
    "        interior_loader = DataLoader(interior_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Get full boundary data for current rho\n",
    "        (x_left, t_left, rho_left,\n",
    "         x_right, t_right, rho_right,\n",
    "         x_upper, t_upper, rho_upper,\n",
    "         x_lower, t_lower, rho_lower) = dataset.get_boundary_points(rho)\n",
    "        u_left, u_right, u_upper, u_lower = dataset.get_boundary_values(rho)\n",
    "\n",
    "        x_left, t_left, rho_left = x_left.detach(), t_left.detach(), rho_left.detach()\n",
    "        u_left, u_right, u_upper, u_lower = u_left.detach(), u_right.detach(), u_upper.detach(), u_lower.detach()\n",
    "        \n",
    "        for bx, bt, brho in interior_loader:\n",
    "            # Ensure mini-batch inputs require gradients for differentiation\n",
    "            bx.requires_grad_()\n",
    "            bt.requires_grad_()\n",
    "\n",
    "            def closure():\n",
    "                optim.zero_grad()\n",
    "\n",
    "                # Forward pass on interior mini-batch\n",
    "                pred_res = model(bx, bt, brho)\n",
    "                # Compute derivative with respect to time (no retain_graph here)\n",
    "                u_t = torch.autograd.grad(pred_res, bt, \n",
    "                                           grad_outputs=torch.ones_like(pred_res),\n",
    "                                           retain_graph=True,\n",
    "                                           create_graph=True)[0]\n",
    "                loss_res = torch.mean((u_t - rho * pred_res * (1 - pred_res)) ** 2)\n",
    "\n",
    "                # Forward pass on full-boundary data\n",
    "                pred_left  = model(x_left, t_left, rho_left)\n",
    "                pred_right = model(x_right, t_right, rho_right)\n",
    "                pred_upper = model(x_upper, t_upper, rho_upper)\n",
    "                pred_lower = model(x_lower, t_lower, rho_lower)\n",
    "                loss_bc = torch.mean((pred_upper - pred_lower) ** 2)\n",
    "                loss_ic = torch.mean((pred_left[:, 0] - u_left[:, 0]) ** 2)\n",
    "\n",
    "                loss = loss_res + loss_bc + loss_ic\n",
    "                loss.backward(retain_graph=True)\n",
    "                return loss\n",
    "\n",
    "            loss_val = optim.step(closure)\n",
    "            bx, bt, brho = bx.detach(), bt.detach(), brho.detach()\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"rho\": rho,\n",
    "                \"loss\": loss_val.item()\n",
    "            })\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} complete, Loss: {loss_val.item():.4f}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Print final losses for each rho value\n",
    "last_iteration = total_i - 1\n",
    "for rho in rho_values:\n",
    "    print('Rho: {:4f}, Loss Res: {:4f}, Loss BC: {:4f}, Loss IC: {:4f}'.format(\n",
    "        rho,\n",
    "        loss_track[last_iteration][rho][\"loss_res\"],\n",
    "        loss_track[last_iteration][rho][\"loss_bc\"],\n",
    "        loss_track[last_iteration][rho][\"loss_ic\"]\n",
    "    ))\n",
    "    total_loss = (loss_track[last_iteration][rho][\"loss_res\"] +\n",
    "                  loss_track[last_iteration][rho][\"loss_bc\"] +\n",
    "                  loss_track[last_iteration][rho][\"loss_ic\"])\n",
    "    print('Train Loss: {:4f}'.format(total_loss))\n",
    "torch.save(model.state_dict(), './1dreaction_pinnsformer_params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Evaluation\n",
    "errors = []\n",
    "for rho in rho_values:\n",
    "    res_test, x_test, t_test, rho_test = dataset.get_test_points(rho)\n",
    "    u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(101, 101)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "        pred = pred.cpu().detach().numpy().reshape(101, 101)\n",
    "\n",
    "    rl1 = np.sum(np.abs(u_analytical - pred)) / np.sum(np.abs(u_analytical))\n",
    "    rl2 = np.sqrt(np.sum((u_analytical - pred)**2) / np.sum(u_analytical**2))\n",
    "    print(f\"Rho: {rho}, Relative L1 error: {rl1:.4f}, Relative L2 error: {rl2:.4f}\")\n",
    "    errors.append((rho, rl1, rl2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Visualization\n",
    "for rho in rho_values:\n",
    "    res_test, x_test, t_test, rho_test = dataset.get_test_points(rho)\n",
    "    u_analytical = dataset.analytical_solution(x_test, t_test, rho).cpu().detach().numpy().reshape(101, 101)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_test.to(device), t_test.to(device), rho_test.to(device))[:, 0:1]\n",
    "        pred = pred.cpu().detach().numpy().reshape(101, 101)\n",
    "    abs_error = np.abs(u_analytical - pred)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(pred, extent=[0, 2*np.pi, 1, 0], aspect='auto')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title(f'Predicted u(x,t) for rho={rho}')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./1dreaction_pinnsformer_params_pred_rho_{rho}.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(u_analytical, extent=[0, 2*np.pi, 1, 0], aspect='auto')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title(f'Exact u(x,t) for rho={rho}')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./1dreaction_exact_rho_{rho}.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(abs_error, extent=[0, 2*np.pi, 1, 0], aspect='auto')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title(f'Absolute Error for rho={rho}')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./1dreaction_pinnsformer_params_error_rho_{rho}.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
